[
  {
    "objectID": "posts/week2/post.html",
    "href": "posts/week2/post.html",
    "title": "FastAI Lesson 2",
    "section": "",
    "text": "This lesson was all about getting a model deployed to production. It was an occasion for me to make sure I had jupyter working locally and to understand how that flow would works. I used Jeremy‚Äôs technique of building an app in notebook and exporting it with nbdev directives, which is super neat! I also set myself up on paperspace, which I‚Äôd highly endorese. Eight dollars well spent.\nThis lesson‚Äôs clean notebook was basically a recap of lesson 1, with bears instead of cats and dogs or birds.\nI deployed my model to a Huggingface Space using Gradio. This is based on a model I trained last week for recognizing bean lesions. So if you happen to be a farmer wanting to evaluated the health of your beans‚Ä¶you know who to call üò∏. I skipped the step of using the api for this app. Gradio now has an API client that has to be installed with npm. Which‚Ä¶Im not trying to deploy anything that needs a build step. I wish they hadn‚Äôt done that.\nI hit a few hiccups which were easily navigated with some googling. Mainly these were API changes in various libraries. Inevitable as things move along but bumps nonetheless.\n\n\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. Not great at images that differ in stylisitically. The camera to recognize the bears might be mounted upside down.\nWhere do text models currently have a major deficiency? They‚Äôre plausible, but are they correct?!\nWhat are possible negative societal implications of text generation models? Spreading false information that‚Äôs very convincing\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Human in the loop.\nWhat kind of tabular data is deep learning particularly good at? where the column data might be very diverse\nWhat‚Äôs a key downside of directly using a deep learning model for recommendation systems? The recommendations might not actually be helpful. eg. they might recommend books you already have\nWhat are the steps of the Drivetrain Approach? objectives, levers, data, model (how the levers influence the objectives)\nHow do the steps of the Drivetrain Approach map to a recommendation system? objective: drive more sales levers: ranking of the recommendations data: past choices of the user model: two models that are contingent on seeing or not seeing the recommendation\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders? Generic class for getting data into a learner\nWhat four things do we need to tell fastai to create DataLoaders?\ninput and output types\nhow get get the items\nhow to label the items\nhow to create a validaton set\nWhat does the splitter parameter to DataBlock do? telling fai how to split of a validation set\nHow do we ensure a random split always gives the same validation set? seed will set the randomness\nWhat letters are often used to signify the independent and dependent variables? x for independent y for dependent\nWhat‚Äôs the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? crop = cut pad = fill in with black squish = distort to fit chose might be dictated by the particulars of the data\nWhat is data augmentation? Why is it needed? creating random variations that seem different (to a model) but don‚Äôt actually change in meaning\nWhat is the difference between item_tfms and batch_tfms? item is single, batch is a group\nWhat is a confusion matrix? a nXn matrix that plots what the model was predicted vs the correct labels. the center diagonal is correct guess. others a re off somehow. lets us see issues in data or model. uses the validation set.\nWhat does export save? a pkl file of the trained model\nWhat is it called when we use a model for getting predictions, instead of training? inference\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better? cpu is more cost effective, easier to manage, more available, and perfectly suitable for running inferences.\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? network issues, latency, sensitive data, complexity of runnning infra\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? data is video vs pictures, bears in are in novel positions or lighting, night pictures, speed of results\nWhat is ‚Äúout-of-domain data‚Äù? data that differs a lot to what was seen in training\nWhat is ‚Äúdomain shift‚Äù? type of data changes over time so the initial model doesnt apply so much. the use of the model actually changes things, so the model has to be adjusted.\nWhat are the three steps in the deployment process?\nmanual steps, human checks it all\nlimited scope. time or geography limited. careful supervisions\ngradual expansion. need reporting. need to consider what can go wrong."
  },
  {
    "objectID": "posts/week2/post.html#quiz",
    "href": "posts/week2/post.html#quiz",
    "title": "FastAI Lesson 2",
    "section": "",
    "text": "Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. Not great at images that differ in stylisitically. The camera to recognize the bears might be mounted upside down.\nWhere do text models currently have a major deficiency? They‚Äôre plausible, but are they correct?!\nWhat are possible negative societal implications of text generation models? Spreading false information that‚Äôs very convincing\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Human in the loop.\nWhat kind of tabular data is deep learning particularly good at? where the column data might be very diverse\nWhat‚Äôs a key downside of directly using a deep learning model for recommendation systems? The recommendations might not actually be helpful. eg. they might recommend books you already have\nWhat are the steps of the Drivetrain Approach? objectives, levers, data, model (how the levers influence the objectives)\nHow do the steps of the Drivetrain Approach map to a recommendation system? objective: drive more sales levers: ranking of the recommendations data: past choices of the user model: two models that are contingent on seeing or not seeing the recommendation\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders? Generic class for getting data into a learner\nWhat four things do we need to tell fastai to create DataLoaders?\ninput and output types\nhow get get the items\nhow to label the items\nhow to create a validaton set\nWhat does the splitter parameter to DataBlock do? telling fai how to split of a validation set\nHow do we ensure a random split always gives the same validation set? seed will set the randomness\nWhat letters are often used to signify the independent and dependent variables? x for independent y for dependent\nWhat‚Äôs the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? crop = cut pad = fill in with black squish = distort to fit chose might be dictated by the particulars of the data\nWhat is data augmentation? Why is it needed? creating random variations that seem different (to a model) but don‚Äôt actually change in meaning\nWhat is the difference between item_tfms and batch_tfms? item is single, batch is a group\nWhat is a confusion matrix? a nXn matrix that plots what the model was predicted vs the correct labels. the center diagonal is correct guess. others a re off somehow. lets us see issues in data or model. uses the validation set.\nWhat does export save? a pkl file of the trained model\nWhat is it called when we use a model for getting predictions, instead of training? inference\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better? cpu is more cost effective, easier to manage, more available, and perfectly suitable for running inferences.\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? network issues, latency, sensitive data, complexity of runnning infra\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? data is video vs pictures, bears in are in novel positions or lighting, night pictures, speed of results\nWhat is ‚Äúout-of-domain data‚Äù? data that differs a lot to what was seen in training\nWhat is ‚Äúdomain shift‚Äù? type of data changes over time so the initial model doesnt apply so much. the use of the model actually changes things, so the model has to be adjusted.\nWhat are the three steps in the deployment process?\nmanual steps, human checks it all\nlimited scope. time or geography limited. careful supervisions\ngradual expansion. need reporting. need to consider what can go wrong."
  },
  {
    "objectID": "posts/lesson 4/post.html",
    "href": "posts/lesson 4/post.html",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "Dung beetles can lift 1141 times their own body weight.\n\n\n\n\n\nThis lesson was about NLP. The video differs from the book, in that it uses the huggingface (ü§ó) transformers library. This library provides a convenient mid-level api for working with models. In the lesson there were a couple salient point: - Using a language model for classification (or sequence classification or sentiment analysis). Basically this is about outputting a category label given an input document. - Thinking about how to turn things into classification problems. The basic instance of this type of problem would be sentiment analysis (ie ‚Äòis this review favorable or not?‚Äô). In the case of the example data, we‚Äôre trying to label patent categories&lt;&gt;descriptions tuples as being more or less similar. - Constructing appropriate training, validation, and test sets. - Using pandas dataframes. - Pearson coefficient for calculating metrics. We use this because that‚Äôs how kaggle calculates the contest. - The tokenization process and how that all works (basically splitting up the input document into a list of tokens and then turning those tokens into numbers). This answers the question of how a mathematical function can operate on a document.\nI played with reproducing the results using a different model, distilbert-base-uncased, which is the ü§ó default model. It took a while to massage our previous metrics function to work. They have an evaluate library which I used. My results were worse and took longer to train, but it was useful to go through the exercise. I played with a few things like feature engineering (creating the input documents in various ways) and training for longer, but I was never able to match the results. I also played with using a specifically patent trained language model for this. That work is in process because it involves converting a AutoModelForSeq2SeqLM model into a classifier model. That‚Äôs a project I want to understand a little more deeply and will write about when I get it to work. Chatgpt gave me some help, but I think I‚Äôll experiment with copilot to see what it can do. It seems to involve adding a classification layer which takes the outputs of the model and reduces them down to N categories.\nHere‚Äôs what I‚Äôve got so far. Its not working yet üòè:\n\nKaggle Notebook\n\n\n\n\n\nWhat is ‚Äúself-supervised learning‚Äù? Learning where there are no labels, per-se\nWhat is a ‚Äúlanguage model‚Äù? A language model is a model that predicts the next word in a sequence of words.\nWhy is a language model considered self-supervised? Because it is trained on a corpus of text, and the labels are the next word in the sequence. This means the labels are the input text, just shifted by one word. We don‚Äôt have to explicitey provide labels.\nWhat are self-supervised models usually used for? They are used to create a representation of the input data that can be used for other tasks.\nWhy do we fine-tune language models? Because the language model is trained on a corpus of text that is different from the corpus of text we want to use it on. So we fine-tune it on the corpus of text we want to use it on.\nWhat are the three steps to create a state-of-the-art text classifier?\n\nFine-tune a language model on a dataset of unlabeled text\nFine-tune the language model on your labeled dataset\nUse the fine-tuned language model to train a classifier\n\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? This gives the language model context specific knowledge about the language used in movie reviews.\nWhat are the three steps to prepare your data for a language model?\n\nTokenize the text\nNumericalize the tokens\nCreate batches\n\nWhat is ‚Äútokenization‚Äù? Why do we need it? Tokenization is the process of splitting a document into a list of tokens. We need it because we can‚Äôt feed text into a neural network, we need to feed numbers.\nName three different approaches to tokenization.\n\nSplit on spaces\nSplit on characters\nUse a library like spaCy\n\nWhat is xxbos? It is a token that indicates the beginning of a text.\nList four rules that fastai applies to text during tokenization.\n\nAll text is lowercased\nAll punctuation is replaced with tokens\nAll numbers are replaced with a special token\nWords are replaced with a token if they are not in the vocabulary\n\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that‚Äôs repeated? Because it is a common pattern in text, and it is useful to have a token for it. It also compacts the vocabulary.\nWhat is ‚Äúnumericalization‚Äù? Numericalization is the process of mapping tokens to integers.\nWhy might there be words that are replaced with the ‚Äúunknown word‚Äù token? Because they are not in the vocabulary.\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book‚Äôs website.) The second row of the tensor contains the second batch. The first row of the second batch contains the 65th to 128th tokens.\nWhy do we need padding for text classification? Why don‚Äôt we need it for language modeling? We need padding for text classification because we need to have the same length for each input. We don‚Äôt need it for language modeling because we are predicting the next word in the sequence, so we don‚Äôt need to have the same length for each input.\nWhat does an embedding matrix for NLP contain? What is its shape? An embedding matrix contains the embeddings for each token in the vocabulary. It has a shape of (vocab_size, embedding_size).\nWhat is ‚Äúperplexity‚Äù? Perplexity is a measure of how well a probability distribution or probability model predicts a sample.\nWhy do we have to pass the vocabulary of the language model to the classifier data block? Because we want to use the same vocabulary for the classifier as we used for the language model.\nWhat is ‚Äúgradual unfreezing‚Äù? Gradual unfreezing is the process of unfreezing one layer at a time and training the model.\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts? Because generation can be trained using a classifier, but identification can‚Äôt be trained using a generator."
  },
  {
    "objectID": "posts/lesson 4/post.html#tenacious-animal",
    "href": "posts/lesson 4/post.html#tenacious-animal",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "Dung beetles can lift 1141 times their own body weight."
  },
  {
    "objectID": "posts/lesson 4/post.html#recap",
    "href": "posts/lesson 4/post.html#recap",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "This lesson was about NLP. The video differs from the book, in that it uses the huggingface (ü§ó) transformers library. This library provides a convenient mid-level api for working with models. In the lesson there were a couple salient point: - Using a language model for classification (or sequence classification or sentiment analysis). Basically this is about outputting a category label given an input document. - Thinking about how to turn things into classification problems. The basic instance of this type of problem would be sentiment analysis (ie ‚Äòis this review favorable or not?‚Äô). In the case of the example data, we‚Äôre trying to label patent categories&lt;&gt;descriptions tuples as being more or less similar. - Constructing appropriate training, validation, and test sets. - Using pandas dataframes. - Pearson coefficient for calculating metrics. We use this because that‚Äôs how kaggle calculates the contest. - The tokenization process and how that all works (basically splitting up the input document into a list of tokens and then turning those tokens into numbers). This answers the question of how a mathematical function can operate on a document.\nI played with reproducing the results using a different model, distilbert-base-uncased, which is the ü§ó default model. It took a while to massage our previous metrics function to work. They have an evaluate library which I used. My results were worse and took longer to train, but it was useful to go through the exercise. I played with a few things like feature engineering (creating the input documents in various ways) and training for longer, but I was never able to match the results. I also played with using a specifically patent trained language model for this. That work is in process because it involves converting a AutoModelForSeq2SeqLM model into a classifier model. That‚Äôs a project I want to understand a little more deeply and will write about when I get it to work. Chatgpt gave me some help, but I think I‚Äôll experiment with copilot to see what it can do. It seems to involve adding a classification layer which takes the outputs of the model and reduces them down to N categories.\nHere‚Äôs what I‚Äôve got so far. Its not working yet üòè:\n\nKaggle Notebook"
  },
  {
    "objectID": "posts/lesson 4/post.html#quiz",
    "href": "posts/lesson 4/post.html#quiz",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "What is ‚Äúself-supervised learning‚Äù? Learning where there are no labels, per-se\nWhat is a ‚Äúlanguage model‚Äù? A language model is a model that predicts the next word in a sequence of words.\nWhy is a language model considered self-supervised? Because it is trained on a corpus of text, and the labels are the next word in the sequence. This means the labels are the input text, just shifted by one word. We don‚Äôt have to explicitey provide labels.\nWhat are self-supervised models usually used for? They are used to create a representation of the input data that can be used for other tasks.\nWhy do we fine-tune language models? Because the language model is trained on a corpus of text that is different from the corpus of text we want to use it on. So we fine-tune it on the corpus of text we want to use it on.\nWhat are the three steps to create a state-of-the-art text classifier?\n\nFine-tune a language model on a dataset of unlabeled text\nFine-tune the language model on your labeled dataset\nUse the fine-tuned language model to train a classifier\n\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? This gives the language model context specific knowledge about the language used in movie reviews.\nWhat are the three steps to prepare your data for a language model?\n\nTokenize the text\nNumericalize the tokens\nCreate batches\n\nWhat is ‚Äútokenization‚Äù? Why do we need it? Tokenization is the process of splitting a document into a list of tokens. We need it because we can‚Äôt feed text into a neural network, we need to feed numbers.\nName three different approaches to tokenization.\n\nSplit on spaces\nSplit on characters\nUse a library like spaCy\n\nWhat is xxbos? It is a token that indicates the beginning of a text.\nList four rules that fastai applies to text during tokenization.\n\nAll text is lowercased\nAll punctuation is replaced with tokens\nAll numbers are replaced with a special token\nWords are replaced with a token if they are not in the vocabulary\n\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that‚Äôs repeated? Because it is a common pattern in text, and it is useful to have a token for it. It also compacts the vocabulary.\nWhat is ‚Äúnumericalization‚Äù? Numericalization is the process of mapping tokens to integers.\nWhy might there be words that are replaced with the ‚Äúunknown word‚Äù token? Because they are not in the vocabulary.\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book‚Äôs website.) The second row of the tensor contains the second batch. The first row of the second batch contains the 65th to 128th tokens.\nWhy do we need padding for text classification? Why don‚Äôt we need it for language modeling? We need padding for text classification because we need to have the same length for each input. We don‚Äôt need it for language modeling because we are predicting the next word in the sequence, so we don‚Äôt need to have the same length for each input.\nWhat does an embedding matrix for NLP contain? What is its shape? An embedding matrix contains the embeddings for each token in the vocabulary. It has a shape of (vocab_size, embedding_size).\nWhat is ‚Äúperplexity‚Äù? Perplexity is a measure of how well a probability distribution or probability model predicts a sample.\nWhy do we have to pass the vocabulary of the language model to the classifier data block? Because we want to use the same vocabulary for the classifier as we used for the language model.\nWhat is ‚Äúgradual unfreezing‚Äù? Gradual unfreezing is the process of unfreezing one layer at a time and training the model.\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts? Because generation can be trained using a classifier, but identification can‚Äôt be trained using a generator."
  },
  {
    "objectID": "posts/week1/post.html",
    "href": "posts/week1/post.html",
    "title": "FastAI Week 1",
    "section": "",
    "text": "This week was basically an intro and some quick examples of using the fastai library in a variety of contexts. Most of the initial demonstration comes in the form of computer vision.\nI ran through the clean notebook on Kaggle. I also made my own image classifier that evaluates pictures of bean leaves and labels them with various disease type (or if they‚Äôre healthy). \n\n\nI thought it would be valuable to go through the quiz and document my answers and reasoning (if that applies).\n\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\n\nThese are all false, although common, misconceptions. I‚Äôd say I had these too for a long time. This course convinced me otherwise.\n\nName five areas where deep learning is now the best in the world.\n\nradiology\ngo playing\n\nWhat was the name of the first device that was based on the principle of the artificial neuron? perception\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\n\na set of processing units (what is this?)\na state of activation\nan output function for each unit\na pattern of connectvity (I‚Äôm taking all the above to basically be analagous to a perceptron)\npropogation (how do we ‚Äúpush‚Äù activities into the model to train it)\nactivation rule (how multiple produce an output)\nlearning rule (how do we modify a network of perceptrons by ‚Äúexperience‚Äù)\nenvironment\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\n\npeople missed minsky‚Äôs second insight that although a single layer could not approximate functions, multiple layers could\nhowever multilayered networks were often too slow to be useful (until GPUs)\n\n\nWhat is a GPU?\n\n\nA matrix multiplying chip (originballyt designed for graphics)\n\n\nOpen a notebook and execute a cell containing: 1+1. What happens? 2!\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. https://www.kaggle.com/code/zanadar/fastai-ch1-clean\nComplete the Jupyter Notebook online appendix. Don‚Äôt know what this is‚Ä¶.\nWhy is it hard to use a traditional computer program to recognize images in a photo? How would you describe the steps of this process? ‚Äúcomputers are giant morons, not giant brains‚Äù\nWhat did Samuel mean by ‚Äúweight assignment‚Äù? weight assignments are the value of variables (weights) that are part of the input to our ML program. they affect its operation.\nWhat term do we normally use in deep learning for what Samuel called ‚Äúweights‚Äù? parameters\nDraw a picture that summarizes Samuel‚Äôs view of a machine learning model. \nWhy is it hard to understand why a deep learning model makes a particular prediction? Sheer volume of model interconnections, parameters. This is called interprability\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? The universal approximation theorem\nWhat do you need in order to train a model? Labeled data\nHow could a feedback loop impact the rollout of a predictive policing model? Arrests/area -&gt; activity is concentrated in that area -&gt; more arrests there. This is the problem of ‚Äúoverfitting‚Äù and happens when a model doesn‚Äôt generalize/\nDo we always have to use 224√ó224-pixel images with the cat recognition model? No but thats a convention\nWhat is the difference between classification and regression? classification: categories. regression: numerical value\nWhat is a validation set? What is a test set? Why do we need them? validation is used to check the model after training and as part of the process of creating the model to make sure our model generalized. Test set is a further ‚Äòsecret‚Äô set of data that is used to check the model at the end.\nWhat will fastai do if you don‚Äôt provide a validation set? Create one on its own\nCan we always use a random sample for a validation set? Why or why not? No.¬†For example in time series we can‚Äôt just randomly select points as they won‚Äôt make sense for the input. we would set aside a validation set thats a portion of the timeline.\nWhat is overfitting? Provide an example. When a model is trained for a long time on data, it can learn to recognize the exact data in the training set and not generalize. For example it would recognize all the individual cats, but not generalze to a new cat.\nWhat is a metric? How does it differ from ‚Äúloss‚Äù? an indication of performance of the model against the validation data. loss can be the same, but it might differ in order to be useful as a function to update the parameters/weights\nHow can pretrained models help? Taking a model where there are existing layers that can do something and specializign the last layer(s) to our specific task. For example earlier layers may be able to recognize shapes, edges, textures, etc. and then we use transfer learning to tune that model to recognize cats and dogs.\nWhat is the ‚Äúhead‚Äù of a model? the top layer\nWhat kinds of features do the early layers of a CNN find? How about the later layers? In a vision model, more basic shapes like edges and gradients. Later is more specific elaborations of those earlier layers, like ‚Äúmultiple layers‚Äù or patterns\nAre image models only useful for photos? No.¬†Different kinds of data can be turned into images and then used to train a model.\nWhat is an ‚Äúarchitecture‚Äù? An organziation of layers within a model, albeit as a template for a function. A model is an architecture + a set of parameters,\nWhat is segmentation? Labelling pixels in an imge.\nWhat is y_range used for? When do we need it? in a collabarative model, with a regression prediction, y_range tells us the range of possible predictions\nWhat are ‚Äúhyperparameters‚Äù? Choices about parameters themselves (such as learning rates and data augmentation strategies), that lend meaning to the weights\nWhat‚Äôs the best way to avoid failures when using AI in an organization? Have a good test dataset to check a model against."
  },
  {
    "objectID": "posts/week1/post.html#quiz",
    "href": "posts/week1/post.html#quiz",
    "title": "FastAI Week 1",
    "section": "",
    "text": "I thought it would be valuable to go through the quiz and document my answers and reasoning (if that applies).\n\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\n\nThese are all false, although common, misconceptions. I‚Äôd say I had these too for a long time. This course convinced me otherwise.\n\nName five areas where deep learning is now the best in the world.\n\nradiology\ngo playing\n\nWhat was the name of the first device that was based on the principle of the artificial neuron? perception\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\n\na set of processing units (what is this?)\na state of activation\nan output function for each unit\na pattern of connectvity (I‚Äôm taking all the above to basically be analagous to a perceptron)\npropogation (how do we ‚Äúpush‚Äù activities into the model to train it)\nactivation rule (how multiple produce an output)\nlearning rule (how do we modify a network of perceptrons by ‚Äúexperience‚Äù)\nenvironment\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\n\npeople missed minsky‚Äôs second insight that although a single layer could not approximate functions, multiple layers could\nhowever multilayered networks were often too slow to be useful (until GPUs)\n\n\nWhat is a GPU?\n\n\nA matrix multiplying chip (originballyt designed for graphics)\n\n\nOpen a notebook and execute a cell containing: 1+1. What happens? 2!\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. https://www.kaggle.com/code/zanadar/fastai-ch1-clean\nComplete the Jupyter Notebook online appendix. Don‚Äôt know what this is‚Ä¶.\nWhy is it hard to use a traditional computer program to recognize images in a photo? How would you describe the steps of this process? ‚Äúcomputers are giant morons, not giant brains‚Äù\nWhat did Samuel mean by ‚Äúweight assignment‚Äù? weight assignments are the value of variables (weights) that are part of the input to our ML program. they affect its operation.\nWhat term do we normally use in deep learning for what Samuel called ‚Äúweights‚Äù? parameters\nDraw a picture that summarizes Samuel‚Äôs view of a machine learning model. \nWhy is it hard to understand why a deep learning model makes a particular prediction? Sheer volume of model interconnections, parameters. This is called interprability\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? The universal approximation theorem\nWhat do you need in order to train a model? Labeled data\nHow could a feedback loop impact the rollout of a predictive policing model? Arrests/area -&gt; activity is concentrated in that area -&gt; more arrests there. This is the problem of ‚Äúoverfitting‚Äù and happens when a model doesn‚Äôt generalize/\nDo we always have to use 224√ó224-pixel images with the cat recognition model? No but thats a convention\nWhat is the difference between classification and regression? classification: categories. regression: numerical value\nWhat is a validation set? What is a test set? Why do we need them? validation is used to check the model after training and as part of the process of creating the model to make sure our model generalized. Test set is a further ‚Äòsecret‚Äô set of data that is used to check the model at the end.\nWhat will fastai do if you don‚Äôt provide a validation set? Create one on its own\nCan we always use a random sample for a validation set? Why or why not? No.¬†For example in time series we can‚Äôt just randomly select points as they won‚Äôt make sense for the input. we would set aside a validation set thats a portion of the timeline.\nWhat is overfitting? Provide an example. When a model is trained for a long time on data, it can learn to recognize the exact data in the training set and not generalize. For example it would recognize all the individual cats, but not generalze to a new cat.\nWhat is a metric? How does it differ from ‚Äúloss‚Äù? an indication of performance of the model against the validation data. loss can be the same, but it might differ in order to be useful as a function to update the parameters/weights\nHow can pretrained models help? Taking a model where there are existing layers that can do something and specializign the last layer(s) to our specific task. For example earlier layers may be able to recognize shapes, edges, textures, etc. and then we use transfer learning to tune that model to recognize cats and dogs.\nWhat is the ‚Äúhead‚Äù of a model? the top layer\nWhat kinds of features do the early layers of a CNN find? How about the later layers? In a vision model, more basic shapes like edges and gradients. Later is more specific elaborations of those earlier layers, like ‚Äúmultiple layers‚Äù or patterns\nAre image models only useful for photos? No.¬†Different kinds of data can be turned into images and then used to train a model.\nWhat is an ‚Äúarchitecture‚Äù? An organziation of layers within a model, albeit as a template for a function. A model is an architecture + a set of parameters,\nWhat is segmentation? Labelling pixels in an imge.\nWhat is y_range used for? When do we need it? in a collabarative model, with a regression prediction, y_range tells us the range of possible predictions\nWhat are ‚Äúhyperparameters‚Äù? Choices about parameters themselves (such as learning rates and data augmentation strategies), that lend meaning to the weights\nWhat‚Äôs the best way to avoid failures when using AI in an organization? Have a good test dataset to check a model against."
  },
  {
    "objectID": "posts/lesson 5/post.html",
    "href": "posts/lesson 5/post.html",
    "title": "FastAI Lesson 5",
    "section": "",
    "text": "Polar bears can wait for hours or even days to catch their prey.\n\n\n\n\n\nThis lesson was about tabular data. Firstly we looked at recreating a simple linear model in python. This used sum product with a series of coefficients broadcast across our dataset. We used SGD to optimize this. Then we looked at creating a simple neural network to do the same thing. This was a single hidden layer with a single output layer. We used pytorch for this and optimized with SGD using the gradients on the tensors. Finally we looked at constructing the most basic ‚Äúdeep‚Äù neural network. This was a series of hidden layers, a relu, and an output layer. This was very fiddly in terms of parameters, the size of things, how they were initialized. This motivated the need for a higher level library like fastai, which was demonstrated in the second notebook. This took about 5 lines of code to do the same thing as the previous notebook.\nFinally Jeremy introduced a few new concepts for machine learning, specifically binary splits and random forests. The idea of a binary split is to find some fact about your data that divides it in two. For example with the titanic dataset, is the person male or female? Then we examine the dependent variable with respect to that split. Given a series of binary splits, getting finer and finer granularity we can build a decision tree. The random forest is a collection of decision trees, each one trained on a subset of the data. The random forest then takes the average of all the trees to make a prediction.\nJeremy recommended using random forests as a way to understand datasets and as an initial baseline when working with tabular data. This is because they are fast to train and can give you a good idea of what is going on with your data, including things like which features are of particular importance (in the titanic dataset, sex is hugely predictive of survival, for example).\n\n\n\n\nWhat is a continuous variable? A variable that can take on any value in a range.\nWhat is a categorical variable? A variable that can take on a limited number of values.\nProvide two of the words that are used for the possible values of a categorical variable. Levels or classes.\nWhat is a ‚Äúdense layer‚Äù? A layer where each input is connected to each output.\nHow do entity embeddings reduce memory usage and speed up neural networks? They replace a categorical variable with a lookup table\nWhat kinds of datasets are entity embeddings especially useful for? Those with high cardinality categorical variables.\nWhat are the two main families of machine learning algorithms? Ensemble of decision trees and neural networks. The former is more interpretable, the latter is more flexible. The former is useful for structured data, the latter is useful for unstructured data.\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas? Because the categorical variable has an intrinsic ordering. For example, the days of the week. We can do this in pandas by using the ordered=True argument when creating the categorical variable.\nSummarize what a decision tree algorithm does. It finds the best binary split for each column in the dataset, and then repeats this process on each of the two resulting datasets, and so on, until it reaches some stopping criterion.\nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? Dates can be treated as continuous or categorical variables, but it is often useful to preprocess them into additional categorical variables such as year, month, day, holidays, and so forth. This is because dates often have special meanings that are not captured by treating them as pure numbers.\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? No, because the data is ordered by date. Instead\nWhat is pickle and what is it useful for? Pickle is a python library for saving python objects to disk. It is useful for saving models, so that they can be loaded later.\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter? mse is the mean squared error of the dependent variable in that group of rows. samples is the number of rows in that group. values is the mean of the dependent variable in that group.\nHow do we deal with outliers, before building a decision tree? We can remove them, or cap them at some maximum or minimum value. Or we can use a log transform to bring them into a more reasonable range.\nHow do we handle categorical variables in a decision tree? We can use one-hot encoding (which isn‚Äôt very necessary) or just keep them as is, making split decisions based on whether a row has a particular value or not.\nWhat is bagging? Bagging is the process of training a model on a subset of the data, and then averaging the results. This is useful for reducing overfitting.\nWhat is the difference between max_samples and max_features when creating a random forest? max_samples is the number of rows to use for each tree. max_features is the number of columns to use for each split.\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? Yes, because the model will be able to memorize the training set given so many trees. It is important to use a validation set to find the optimal number of trees and to make sure the model will generalize.\nIn the section ‚ÄúCreating a Random Forest‚Äù, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest? Because the random forest is just the average of the trees, and preds.mean(0) is the average of the predictions of the trees.\nWhat is ‚Äúout-of-bag-error‚Äù? The error of the model on the rows that were not used for training that particular tree.\nMake a list of reasons why a model‚Äôs validation set error might be worse than the OOB error. How could you test your hypotheses? Because the validation set might be distributed differently than the training set. We can test this by looking at the feature importance of each tree.\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data?\n\nBecause we can use the predictions of each tree to calculate the standard deviation of the predictions.\n\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n\nBecause we can look at the feature importance of each tree.\n\nWhich columns are the strongest predictors?\n\nBecause we can look at the feature importance of each tree.\n\nHow do predictions vary as we vary these columns?\n\nBecause we can look at the feature importance of each tree.\n\n\nWhat‚Äôs the purpose of removing unimportant variables? To make the model simpler and faster to train, and to reduce overfitting.\nWhat‚Äôs a good type of plot for showing tree interpreter results? A waterfall plot.\nWhat is the ‚Äúextrapolation problem‚Äù? When a model is asked to make predictions for data that is outside the range of the training data.\nHow can you tell if your test or validation set is distributed in a different way than your training set? By looking at the feature importance of each tree.\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? Because it is a date, and dates have a special meaning that is not captured by treating them as pure numbers.\nWhat is ‚Äúboosting‚Äù? Boosting is the process of training a series of models, each one trying to correct the errors of the previous one.\nHow could we use embeddings with a random forest? Would we expect this to help? We could use embeddings with a random forest by replacing each categorical variable with a one-hot encoded vector, and then using the embeddings of those vectors as the input to the random forest. This would probably not help, because random forests are already very good at dealing with categorical variables.\nWhy might we not always use a neural net for tabular modeling? Because neural nets are more complex and take longer to train than random forests, and random forests are often just as accurate."
  },
  {
    "objectID": "posts/lesson 5/post.html#tenacious-animal",
    "href": "posts/lesson 5/post.html#tenacious-animal",
    "title": "FastAI Lesson 5",
    "section": "",
    "text": "Polar bears can wait for hours or even days to catch their prey."
  },
  {
    "objectID": "posts/lesson 5/post.html#recap",
    "href": "posts/lesson 5/post.html#recap",
    "title": "FastAI Lesson 5",
    "section": "",
    "text": "This lesson was about tabular data. Firstly we looked at recreating a simple linear model in python. This used sum product with a series of coefficients broadcast across our dataset. We used SGD to optimize this. Then we looked at creating a simple neural network to do the same thing. This was a single hidden layer with a single output layer. We used pytorch for this and optimized with SGD using the gradients on the tensors. Finally we looked at constructing the most basic ‚Äúdeep‚Äù neural network. This was a series of hidden layers, a relu, and an output layer. This was very fiddly in terms of parameters, the size of things, how they were initialized. This motivated the need for a higher level library like fastai, which was demonstrated in the second notebook. This took about 5 lines of code to do the same thing as the previous notebook.\nFinally Jeremy introduced a few new concepts for machine learning, specifically binary splits and random forests. The idea of a binary split is to find some fact about your data that divides it in two. For example with the titanic dataset, is the person male or female? Then we examine the dependent variable with respect to that split. Given a series of binary splits, getting finer and finer granularity we can build a decision tree. The random forest is a collection of decision trees, each one trained on a subset of the data. The random forest then takes the average of all the trees to make a prediction.\nJeremy recommended using random forests as a way to understand datasets and as an initial baseline when working with tabular data. This is because they are fast to train and can give you a good idea of what is going on with your data, including things like which features are of particular importance (in the titanic dataset, sex is hugely predictive of survival, for example)."
  },
  {
    "objectID": "posts/lesson 5/post.html#quiz",
    "href": "posts/lesson 5/post.html#quiz",
    "title": "FastAI Lesson 5",
    "section": "",
    "text": "What is a continuous variable? A variable that can take on any value in a range.\nWhat is a categorical variable? A variable that can take on a limited number of values.\nProvide two of the words that are used for the possible values of a categorical variable. Levels or classes.\nWhat is a ‚Äúdense layer‚Äù? A layer where each input is connected to each output.\nHow do entity embeddings reduce memory usage and speed up neural networks? They replace a categorical variable with a lookup table\nWhat kinds of datasets are entity embeddings especially useful for? Those with high cardinality categorical variables.\nWhat are the two main families of machine learning algorithms? Ensemble of decision trees and neural networks. The former is more interpretable, the latter is more flexible. The former is useful for structured data, the latter is useful for unstructured data.\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas? Because the categorical variable has an intrinsic ordering. For example, the days of the week. We can do this in pandas by using the ordered=True argument when creating the categorical variable.\nSummarize what a decision tree algorithm does. It finds the best binary split for each column in the dataset, and then repeats this process on each of the two resulting datasets, and so on, until it reaches some stopping criterion.\nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? Dates can be treated as continuous or categorical variables, but it is often useful to preprocess them into additional categorical variables such as year, month, day, holidays, and so forth. This is because dates often have special meanings that are not captured by treating them as pure numbers.\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? No, because the data is ordered by date. Instead\nWhat is pickle and what is it useful for? Pickle is a python library for saving python objects to disk. It is useful for saving models, so that they can be loaded later.\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter? mse is the mean squared error of the dependent variable in that group of rows. samples is the number of rows in that group. values is the mean of the dependent variable in that group.\nHow do we deal with outliers, before building a decision tree? We can remove them, or cap them at some maximum or minimum value. Or we can use a log transform to bring them into a more reasonable range.\nHow do we handle categorical variables in a decision tree? We can use one-hot encoding (which isn‚Äôt very necessary) or just keep them as is, making split decisions based on whether a row has a particular value or not.\nWhat is bagging? Bagging is the process of training a model on a subset of the data, and then averaging the results. This is useful for reducing overfitting.\nWhat is the difference between max_samples and max_features when creating a random forest? max_samples is the number of rows to use for each tree. max_features is the number of columns to use for each split.\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? Yes, because the model will be able to memorize the training set given so many trees. It is important to use a validation set to find the optimal number of trees and to make sure the model will generalize.\nIn the section ‚ÄúCreating a Random Forest‚Äù, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest? Because the random forest is just the average of the trees, and preds.mean(0) is the average of the predictions of the trees.\nWhat is ‚Äúout-of-bag-error‚Äù? The error of the model on the rows that were not used for training that particular tree.\nMake a list of reasons why a model‚Äôs validation set error might be worse than the OOB error. How could you test your hypotheses? Because the validation set might be distributed differently than the training set. We can test this by looking at the feature importance of each tree.\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data?\n\nBecause we can use the predictions of each tree to calculate the standard deviation of the predictions.\n\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n\nBecause we can look at the feature importance of each tree.\n\nWhich columns are the strongest predictors?\n\nBecause we can look at the feature importance of each tree.\n\nHow do predictions vary as we vary these columns?\n\nBecause we can look at the feature importance of each tree.\n\n\nWhat‚Äôs the purpose of removing unimportant variables? To make the model simpler and faster to train, and to reduce overfitting.\nWhat‚Äôs a good type of plot for showing tree interpreter results? A waterfall plot.\nWhat is the ‚Äúextrapolation problem‚Äù? When a model is asked to make predictions for data that is outside the range of the training data.\nHow can you tell if your test or validation set is distributed in a different way than your training set? By looking at the feature importance of each tree.\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? Because it is a date, and dates have a special meaning that is not captured by treating them as pure numbers.\nWhat is ‚Äúboosting‚Äù? Boosting is the process of training a series of models, each one trying to correct the errors of the previous one.\nHow could we use embeddings with a random forest? Would we expect this to help? We could use embeddings with a random forest by replacing each categorical variable with a one-hot encoded vector, and then using the embeddings of those vectors as the input to the random forest. This would probably not help, because random forests are already very good at dealing with categorical variables.\nWhy might we not always use a neural net for tabular modeling? Because neural nets are more complex and take longer to train than random forests, and random forests are often just as accurate."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Giant Morons: a blog about deep learning üî¨",
    "section": "",
    "text": "Computers, as any programmer will tell you, are giant morons, not giant brains - Arthur Samuel\n\nThoughts, notes, experiments, and useful tidbits I find in my deep learning journey.\nI‚Äôm Zander, a father, a learner, and ever-curious software engineer. I‚Äôm captivated by recent AI developments. This blog aims to document my journey from zero to ML/deep learning practioner.\nAlong the way I‚Äôll be sharing useful things I learn and publicly documenting of my work. I‚Äôll also be making bad jokes. Hope you‚Äôre okay with that.\nThe banner image for this blog was created with dall-e"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Giant Morons",
    "section": "",
    "text": "FastAI lesson 7\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 7\n\n\n\n\n\n\nNov 30, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 6\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 6\n\n\n\n\n\n\nNov 29, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 5\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 5\n\n\n\n\n\n\nNov 28, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 4\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 4\n\n\n\n\n\n\nNov 26, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nA Checkin on what‚Äôs working\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nSome observations and thoughts about my learning process so far\n\n\n\n\n\n\nNov 17, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 3\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 3\n\n\n\n\n\n\nNov 15, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 2\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 2\n\n\n\n\n\n\nNov 13, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Week 1\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for week 1\n\n\n\n\n\n\nNov 7, 2023\n\n\nZander Mackie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lesson 6/post.html",
    "href": "posts/lesson 6/post.html",
    "title": "FastAI Lesson 6",
    "section": "",
    "text": "It‚Äôs important to note that while tardigrades are incredibly resilient, they are not invincible, and there are conditions and stressors that can ultimately kill them.\n\n\n\n\n\nThis lesson was a continuation of some of the ideas of the previous lesson about tabular data (I sort of conflated these two in my previous recap). It also continued some of the earlier lesson on computer vision, demonstrating some new techniques like ensembling. Introduced convnext model, which is an improvement in accuracy and memory and speed on imagenet. Talked about some crucial techniques for dealing with less performant training situations and larger models, including gradient accumulation. This technique updated the model weights continuously during an epoch per-batch. This allows training larger models without needing a GPU with more RAM, which is kinda neat!\nJeremy also introduced some techniques for competing on Kaggle, emphasizing that the most crucial things are a) creating a good test set b) iteration speed. This jives with my intuitions and experience with developing software and my own work. In situations where iterating is slow, it becomes incredibly hard to make forward progress. The anecdote I‚Äôve heard about this is regarding an art class (pottery or photography): the students who make lots of things imperfectly, rather than focussing on perfecting a single thing, go on to build mastery and create the most artful work. Jeremy shared a story about a highly decorated practitioner focussing on making a single perfect model, but coming in literal last place because they failed to iterate.\n\n\n\nI got chatGPT to generate some quiz questions for me because this material covered the same book chapter as the last lesson:\n\n\n\nTwoR Model\n\nWhat is the TwoR model, and how does it differ from more complex machine learning models? TwoR is a model that uses two rules to make predictions. It is a simple model that is easy to understand and explain, but it is not very accurate. Another name for it is binary split.\n\nCreating a Decision Tree\n\nDescribe the basic process of creating a decision tree. What are the key steps involved? Evaluate groups of data and split them into smaller groups based on the most important feature. Repeat this process until the groups are small enough to make predictions.\n\nGini Impurity\n\nWhat is Gini impurity, and how is it used in the context of decision trees? Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. It is used to determine which feature to split on.\n\nMaking a Submission\n\nWhat are the key considerations to keep in mind when preparing to make a submission in a data science competition? Make sure the submission is in the correct format and that the data is in the correct order.\n\nBagging\n\nExplain the concept of bagging in machine learning. How does it contribute to the performance of ensemble models? Bagging is a method of combining the results of multiple models trained on different subsets of the same data. It reduces variance and helps to avoid overfitting.\n\nRandom Forest Introduction\n\nWhat is a random forest, and how does it improve upon the idea of a single decision tree? A random forest is an ensemble learning method that uses a collection of decision trees to make predictions. It improves upon a single decision tree by reducing variance and avoiding overfitting.\n\nCreating a Random Forest\n\nOutline the steps involved in creating a random forest model.\n\n\nCreate a collection of decision trees.\nTrain each tree on a different subset of the data.\nMake predictions using the average of the predictions from all the trees.\n\nFeature Importance in Random Forest\n\nHow is feature importance determined in a random forest, and why is it useful? Feature importance is determined by calculating the average decrease in gini impurity across all trees in the forest. It is useful for determining which features are most important in making predictions.\n\nAdding Trees to a Random Forest\n\nWhat are the effects of adding more trees to a random forest model? Adding more trees to a random forest model increases the accuracy of the model, but it also increases the time required to train the model.\n\nOut-of-Bag (OOB) Error\n\nWhat is OOB error in the context of random forests, and how is it calculated? OOB error is the error rate of a random forest model on the training set. The validation set in bagging is all the data not included in the training set. It is calculated by averaging the error rates of all trees in the forest.\n\nModel Interpretation\n\nDiscuss the importance and methods of model interpretation in machine learning. Model interpretation is important because it allows us to understand how a model works and why it makes the predictions it does. It can be done by looking at feature importance, partial dependence, and SHAP values. This is easier in the case of a random forest than a nueral network.\n\nRemoving Redundant Features\n\nWhy is it important to remove redundant features from a model, and how can it be achieved? It is important to remove redundant features from a model because they can cause overfitting and reduce the accuracy of the model. This can be achieved by looking at feature importance and removing features with low importance.\n\nPartial Dependence\n\nExplain what partial dependence is and how it helps in understanding machine learning models. Partial dependence is a method of interpreting machine learning models by looking at the relationship between a feature and the target variable while holding all other features constant. It helps us understand how a model works and why it makes the predictions it does.\n\nExplaining Predictions\n\nHow can you explain why a particular prediction was made by a machine learning model? You can explain why a particular prediction was made by a machine learning model by looking at feature importance, partial dependence, and SHAP values.\n\nOverfitting in Random Forest\n\nIs it possible to overfit a random forest? Under what circumstances might this occur? Yes, it is possible to overfit a random forest. This can occur when the trees in the forest are too deep, ie when there are too many leaf nodes. This happens because the trees are too specific and don‚Äôt generalize well to new data.\n\nGradient Boosting\n\nWhat is gradient boosting, and how does it differ from other ensemble methods like random forests? Gradient boosting is an ensemble method that uses a collection of weak learners to make predictions. It differs from other ensemble methods like random forests because it uses a collection of weak learners instead of a collection of decision trees.\n\nfastkaggle\n\nWhat is fastkaggle, and how does it assist in Kaggle competitions? fastkaggle is a library that makes it easier to use Kaggle datasets in fastai. It assists in Kaggle competitions by providing a way to download and use Kaggle datasets in fastai. It also provides a way to submit predictions to Kaggle.\n\nfastcore.parallel\n\nDescribe the functionality of fastcore.parallel and its application in data processing. fastcore.parallel is a library that makes it easier to use parallel processing in fastai. It can be used to speed up data processing by running multiple processes at once across the cores of a machine.\n\nImage Resizing Methods\n\nExplain the significance of item_tfms=Resize(480, method='squish') in image processing. This is a method of resizing images to a specific size. It is used to make sure that all images are the same size before they are fed into a model.\n\nCriteria for Evaluating Models\n\nWhat are some key criteria for evaluating the performance of machine learning models? Some key criteria for evaluating the performance of machine learning models are accuracy, precision, recall, and F1 score."
  },
  {
    "objectID": "posts/lesson 6/post.html#tenacious-animal",
    "href": "posts/lesson 6/post.html#tenacious-animal",
    "title": "FastAI Lesson 6",
    "section": "",
    "text": "It‚Äôs important to note that while tardigrades are incredibly resilient, they are not invincible, and there are conditions and stressors that can ultimately kill them."
  },
  {
    "objectID": "posts/lesson 6/post.html#recap",
    "href": "posts/lesson 6/post.html#recap",
    "title": "FastAI Lesson 6",
    "section": "",
    "text": "This lesson was a continuation of some of the ideas of the previous lesson about tabular data (I sort of conflated these two in my previous recap). It also continued some of the earlier lesson on computer vision, demonstrating some new techniques like ensembling. Introduced convnext model, which is an improvement in accuracy and memory and speed on imagenet. Talked about some crucial techniques for dealing with less performant training situations and larger models, including gradient accumulation. This technique updated the model weights continuously during an epoch per-batch. This allows training larger models without needing a GPU with more RAM, which is kinda neat!\nJeremy also introduced some techniques for competing on Kaggle, emphasizing that the most crucial things are a) creating a good test set b) iteration speed. This jives with my intuitions and experience with developing software and my own work. In situations where iterating is slow, it becomes incredibly hard to make forward progress. The anecdote I‚Äôve heard about this is regarding an art class (pottery or photography): the students who make lots of things imperfectly, rather than focussing on perfecting a single thing, go on to build mastery and create the most artful work. Jeremy shared a story about a highly decorated practitioner focussing on making a single perfect model, but coming in literal last place because they failed to iterate."
  },
  {
    "objectID": "posts/lesson 6/post.html#quiz",
    "href": "posts/lesson 6/post.html#quiz",
    "title": "FastAI Lesson 6",
    "section": "",
    "text": "I got chatGPT to generate some quiz questions for me because this material covered the same book chapter as the last lesson:\n\n\n\nTwoR Model\n\nWhat is the TwoR model, and how does it differ from more complex machine learning models? TwoR is a model that uses two rules to make predictions. It is a simple model that is easy to understand and explain, but it is not very accurate. Another name for it is binary split.\n\nCreating a Decision Tree\n\nDescribe the basic process of creating a decision tree. What are the key steps involved? Evaluate groups of data and split them into smaller groups based on the most important feature. Repeat this process until the groups are small enough to make predictions.\n\nGini Impurity\n\nWhat is Gini impurity, and how is it used in the context of decision trees? Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. It is used to determine which feature to split on.\n\nMaking a Submission\n\nWhat are the key considerations to keep in mind when preparing to make a submission in a data science competition? Make sure the submission is in the correct format and that the data is in the correct order.\n\nBagging\n\nExplain the concept of bagging in machine learning. How does it contribute to the performance of ensemble models? Bagging is a method of combining the results of multiple models trained on different subsets of the same data. It reduces variance and helps to avoid overfitting.\n\nRandom Forest Introduction\n\nWhat is a random forest, and how does it improve upon the idea of a single decision tree? A random forest is an ensemble learning method that uses a collection of decision trees to make predictions. It improves upon a single decision tree by reducing variance and avoiding overfitting.\n\nCreating a Random Forest\n\nOutline the steps involved in creating a random forest model.\n\n\nCreate a collection of decision trees.\nTrain each tree on a different subset of the data.\nMake predictions using the average of the predictions from all the trees.\n\nFeature Importance in Random Forest\n\nHow is feature importance determined in a random forest, and why is it useful? Feature importance is determined by calculating the average decrease in gini impurity across all trees in the forest. It is useful for determining which features are most important in making predictions.\n\nAdding Trees to a Random Forest\n\nWhat are the effects of adding more trees to a random forest model? Adding more trees to a random forest model increases the accuracy of the model, but it also increases the time required to train the model.\n\nOut-of-Bag (OOB) Error\n\nWhat is OOB error in the context of random forests, and how is it calculated? OOB error is the error rate of a random forest model on the training set. The validation set in bagging is all the data not included in the training set. It is calculated by averaging the error rates of all trees in the forest.\n\nModel Interpretation\n\nDiscuss the importance and methods of model interpretation in machine learning. Model interpretation is important because it allows us to understand how a model works and why it makes the predictions it does. It can be done by looking at feature importance, partial dependence, and SHAP values. This is easier in the case of a random forest than a nueral network.\n\nRemoving Redundant Features\n\nWhy is it important to remove redundant features from a model, and how can it be achieved? It is important to remove redundant features from a model because they can cause overfitting and reduce the accuracy of the model. This can be achieved by looking at feature importance and removing features with low importance.\n\nPartial Dependence\n\nExplain what partial dependence is and how it helps in understanding machine learning models. Partial dependence is a method of interpreting machine learning models by looking at the relationship between a feature and the target variable while holding all other features constant. It helps us understand how a model works and why it makes the predictions it does.\n\nExplaining Predictions\n\nHow can you explain why a particular prediction was made by a machine learning model? You can explain why a particular prediction was made by a machine learning model by looking at feature importance, partial dependence, and SHAP values.\n\nOverfitting in Random Forest\n\nIs it possible to overfit a random forest? Under what circumstances might this occur? Yes, it is possible to overfit a random forest. This can occur when the trees in the forest are too deep, ie when there are too many leaf nodes. This happens because the trees are too specific and don‚Äôt generalize well to new data.\n\nGradient Boosting\n\nWhat is gradient boosting, and how does it differ from other ensemble methods like random forests? Gradient boosting is an ensemble method that uses a collection of weak learners to make predictions. It differs from other ensemble methods like random forests because it uses a collection of weak learners instead of a collection of decision trees.\n\nfastkaggle\n\nWhat is fastkaggle, and how does it assist in Kaggle competitions? fastkaggle is a library that makes it easier to use Kaggle datasets in fastai. It assists in Kaggle competitions by providing a way to download and use Kaggle datasets in fastai. It also provides a way to submit predictions to Kaggle.\n\nfastcore.parallel\n\nDescribe the functionality of fastcore.parallel and its application in data processing. fastcore.parallel is a library that makes it easier to use parallel processing in fastai. It can be used to speed up data processing by running multiple processes at once across the cores of a machine.\n\nImage Resizing Methods\n\nExplain the significance of item_tfms=Resize(480, method='squish') in image processing. This is a method of resizing images to a specific size. It is used to make sure that all images are the same size before they are fed into a model.\n\nCriteria for Evaluating Models\n\nWhat are some key criteria for evaluating the performance of machine learning models? Some key criteria for evaluating the performance of machine learning models are accuracy, precision, recall, and F1 score."
  },
  {
    "objectID": "posts/lesson 3/post.html",
    "href": "posts/lesson 3/post.html",
    "title": "FastAI Lesson 3",
    "section": "",
    "text": "This was a tough lesson. I started calling this the ‚Äúweek 3 wall‚Äù because I felt just totally stopped in my tracks by this one. Conceptually it just took me a a long time to wrap my head around this one. One of the things I‚Äôve had to embrace with this course to make progress is just allowing myself to not understand all the details of something. Like for example I get the idea of using a derivative to optimize a quadratic function. Somehow intuitively I can make sense of the image of a tangent line moving down a function and slowly flattening out at the bottom of the graph. But putting all the pieces together has been a struggle. I worked through the titanic example on my own, and that helped it make a bit more sense. What helped there was going from the linear function to the super simple two-layer nueral net. And applying matrix multiplication there was helpful to cement the concept.\nAs Jeremy has said time and again, tenacity is super important. I wasn‚Äôt going to let this week stall me. To inspire myself I generated some fun pictures in Dall-e of especially tenacious animals. Here‚Äôs a honey-badger for you viewing pleasure. I encourage you to adopt it as your fastai spirit animal.\n\n\n\nOne of the crucial things that took me a long time to understand is the relationship between the loss function and the model. For some reason I had the idea that the loss function was related to the model somehow, like in some kind of mathematical sense. But the crucial thing here to understand is that the model (which includes the architecture and the parameters) is tangential to the loss function in SGD. The loss function is a way to understand the performance of the model WRT a known answer. SGD is a method of using the loss function (which is somewhat arbitrary - at least that‚Äôs what it seems like a the moment) to optimize or train the values of the weights of the model. The ‚Äúgradient‚Äù is actually a calculation of the slope of the loss function WRT the input parameters, and gradients might be many gradients - one for each parameter. The gradient is calculated using the derivative of the loss function, which says for a change in the input, how will the loss be changed. What is mysterious to me is the backwards/back propogation step. What‚Äôs ultimately confusing to me is the fact that the loss function helps the input parameters adjust, but the input parameters are not directly involved in the loss function at all! They loss function only uses the predictions and the known answers. So how does the loss function help the parameters adjust? I think this is the backwards/back propogation step, but I‚Äôm not sure. I think I need to do some more reading on this. The key seems to be the ‚Äúchain rule‚Äù.\n\nThe chain rule is a fundamental principle in calculus that states how to compute the derivative of a composite function. Concisely, it can be stated as:\n\n\n‚ÄúIf a function \\(( y )\\) is a composite of another function \\((u)\\), such that \\((y=f(u))\\) and \\((u=g(x))\\), then the derivative of \\(( y )\\) with respect to \\(( x )\\) is the product of the derivative of ( f ) with respect to ( u ) and the derivative of ( g ) with respect to ( x ). In mathematical terms, it is expressed as \\(( \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} )\\).‚Äù\n\nConceptually I can start to grasp that there is function composition going on, but the details are still a bit fuzzy. Ultimately the way I‚Äôm thinking about it is that the prediction is itself the output of a function. But thinking about the prediction as an arbitary set of values isn‚Äôt quite correct. Because I started thinking, well if the prediction is just an array of values, how is it possible to know anything about what produced those values? And in the general sense its not. But this is what pytorch.requires_grad allows us to do. Setting that parameter on the input actually causes the library to records a computation graph and the prediction (or the output of running the params through that graph) will be statefully associated with the input parameters via that graph, which is what allows us to call .backward() on the prediction and have the gradients calculated. Because the prediction has knowledge of what produced it, its possible to then follow that chain of operations backwards, calculating the gradients of each step, and then using those gradients to adjust the parameters. I think this is the key to understanding the backwards/back propogation step.\nGetting the breed recognizer model deployed was a bit trickier than the last model. Due to the use of some of the models from timm, I entered into some fiddly dependency dancing. Luckily other people had figured this out for me and so a quick google (which turned up an answer on the fastai forums) turned up a fix. One of the weirder things that I‚Äôm struggling with in terms of deployment is how to manage dependencies for an app built in this way. Running the app from jupyterhub and running a pip install puts the packages into the conda environment that jupyterhub is running in. This makes it slightly hard to understand the dependencies and respective versions that need to go into the requirements.txt file. Ultimately I had to use --report which gives an output of package versions and then use that to populate the requirements.txt file. I‚Äôm not sure if this is the best way to do this, but it‚Äôs what I‚Äôve got for now. Probably there‚Äôs a a better way to use poetry or venv or something to manage this.\nI also found it valuable to run through pytorchs basic tensor tutorial to wrap my head around this datatype a bit more. I found myself reading the torch/tensor Docs a lot, and noticed that the torch.method() docs usually have examples, which makese it easier to understand things. However playing with the stuff in my terminal was always the most illuminating.\n&gt;&gt;&gt; t = torch.randint(high=10,size=(2,3))\n&gt;&gt;&gt; t\ntensor([[8, 0, 7],\n        [0, 7, 3]])\n&gt;&gt;&gt; t.T\ntensor([[8, 0],\n        [0, 7],\n        [7, 3]])\n&gt;&gt;&gt; t.T.shape\ntorch.Size([3, 2])\n&gt;&gt;&gt; t.shape\ntorch.Size([2, 3])\nIts still totally magical to me, though, that I can actually make these things!\n\n\n\nHow is a grayscale image represented on a computer? How about a color image? Images are represented as matrixes in a computer, with every cell in the matrix representing a pixel. In a grayscale image, the value of the cell is the intensity of the pixel. In a color image, the value of the cell is a vector of three values, representing the intensity of the red, green, and blue channels of the pixel.\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why? This dataset has a csv of labels and Train and ‚Äòvalid‚Äô folder. Within each folder, there is a subfolder for each label, eg /train/3. The data is organized this way because this is a common structure and allows for easy loading (dataLoader can read the parent folder and infer the labels from the subfolders).\nExplain how the ‚Äúpixel similarity‚Äù approach to classifying digits works. This approach approach takes the matrix of threes and calculates a mean matrix. We then compare a given image to this mean image. The comparison is performed using something a function that handles negative numbers but taking the absolute values or squaring the squaring the values. This is necessary because simple summing of the matrix resulting from taking the difference will cause some numbers to cancel each other out. The operation to deal with negative numbers (squaring or absolute value) is performed before calculating the mean. The result of this operation is a single number, which is the ‚Äúdistance‚Äù between the image and the mean image. The image is then classified as the label of the mean image with the smallest distance.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them. A list comprehension is a handy way of dealing with lists in python. It allows you to perform an operation on each element of a list and return a new list. For example, [x^2 for x in range(10) if x%2==1]will return a list of the odd numbers from 0 to 9, doubled.\nWhat is a ‚Äúrank-3 tensor‚Äù? A rank-3 tensor is a tensor with 3 dimensions. A rank-0 tensor is a single number, also called a scalar. A rank-1 tensor is a vector. A rank-2 tensor is a matrix. A rank-3 tensor is a 3-dimensional matrix, meaning a matrix of matrixes.\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape? Rank is the number of dimensions in a tensor. Shape is the size of each dimension. You can get the rank from the shape by counting the number of elements in the shape. For example Tensor([1,2,3,4]).shape will return torch.Size([4]) which has a rank 1.\n\n&gt;&gt;&gt; tensor([1],[1],[1]).shape\ntorch.Size([3, 1]) # rank 2, three rows, one column\n# stacked_threes is a a rank 3 tensor\n# it has shape torch.Size([6131, 28, 28]) \nstacked_threes[0][0][0].ndim # rank 0, ie scalar, ie raw value\nstacked_threes[0][0].ndim # rank 1, ie vector\nstacked_threes[0].ndim # rank 2, ie matrix\nstacked_threes.ndim # rank 3, ie 3-dimensional matrix\n\nWhat are RMSE and L1 norm? These are loss functions. RMSE is the root mean squared error. It is the square root of the mean of the squared differences between the predictions and the actual values. The L1 norm is the mean of the absolute value of the differences between the predictions and the actual values.\n\n# RMSE\ndef rmse(preds, targets):\n    return ((preds-targets)**2).mean().sqrt()\n# L1 norm\ndef l1(preds, targets):\n    return (preds-targets).abs().mean()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Using matrix multiplication. Matrix multiplication is a way of performing a calculation on a matrix of numbers. It is much faster than a python loop because it is implemented in C, which is much faster than python. The operation happens on the GPU, which is designed for this kind of operation.\nCreate a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10)).reshape(3,3)*2\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n&gt;&gt;&gt; a[1:,1:]\ntensor([[10, 12],\n        [16, 18]])\n\nWhat is broadcasting? Broadcasting is the ability of pytorch to perform operations on tensors of different shapes. For example, if you have a rank-1 tensor and a rank-2 tensor, pytorch will automatically expand the rank-1 tensor to match the shape of the rank-2 tensor. This is useful because it allows you to perform operations on tensors of different shapes without having to manually reshape them.\n\n&gt;&gt;&gt; a = torch.tensor([1,2,3])\n&gt;&gt;&gt; b = torch.tensor([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; a+b\ntensor([[2, 4, 6],\n        [5, 7, 9]])\n&gt;&gt;&gt; a.broadcast_to((2,3))\ntensor([[1, 2, 3],\n        [1, 2, 3]])\n&gt;&gt;&gt; b.broadcast_to((1,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]]])\n&gt;&gt;&gt; b.broadcast_to((2,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]],\n        [[1, 2, 3],\n         [4, 5, 6]]])\n\nAre metrics generally calculated using the training set, or the validation set? Why? Metrics are calculated using the validation set. We do this because we want to make sure the model is training to become generalized. If we used the training set, we would be optimizing the model to perform well on the training set, but we wouldn‚Äôt know if it was generalizing well.\nWhat is SGD? SGD is a way of optimizing a function, say for examples \\(y=x^2\\). SGD is a way of finding the minimum of this function. It does this by using the derivative to calculate the gradient, ie the slope at a particular point. The gradient tells us how changing the input parameters will change the output. If the gradient is positive, then increasing the input will increase the output. If the gradient is negative, then increasing the input will decrease the output. The gradient is calculated at a particular point, and then the input is adjusted by a small amount in the opposite direction of the gradient. This is repeated until the gradient is zero, which means that the input is at the minimum of the function. We are trying to minimize the function in our case because the function represents the loss of our model ie the gap between our model‚Äôs prediction and the actual value of the input.\nWhy does SGD use mini-batches? We use mini-batches in order to speed up the calculation in SGD. When we are optimizing, we are usually doing so over a large number of parameters. If we were to calculate the gradient for each parameter, it would take a long time. Instead, we can calculate the gradient for a small batch of parameters (as an average of the items in the batch), and then use that to adjust the parameters in the batch. This is much faster than calculating the gradient for each parameter individually.\nWhat are the seven steps in SGD for machine learning?\n\nInitialize the parameters with random values\nCalculate the predictions\nCalculate the loss\nCalculate the gradients, which approximates how the paramters need to change to reduce the loss\nAdjust the parameters by a small amount in the opposite direction of the gradient. ie If the gradient is positive, then decrease the parameter. If the gradient is negative, then increase the parameter.\nRepeat steps 2-5 until the loss is small enough for our purposes\n\nHow do we initialize the weights in a model? Randomly. That‚Äôs the ‚Äústochastic‚Äù part of SGD. We initialize the weights randomly because we don‚Äôt know what the optimal weights are. We are trying to find them. So we start with random weights and then use SGD to adjust them.\nWhat is ‚Äúloss‚Äù? Loss is a calculation of the difference between the model‚Äôs prediction and the actual value. It is a way of measuring how well the model is performing.\nWhy can‚Äôt we always use a high learning rate? If the learning rate is too high, we will bounce our parameters around, possible never arriving at the minimum. If the learning rate is too low, it will take a long time to arrive at the minimum.\nWhat is a ‚Äúgradient‚Äù? A gradient is a calculation of the slope of a function at a particular point. It tells us how changing the input parameters will change the output. Its the actual specific value of the derivative at a particular point in the loss function.\nDo you need to know how to calculate gradients yourself? Nope, pytorch does it for us.\nWhy can‚Äôt we use accuracy as a loss function? Accuracy is generally steppy. Meaning that an improvement of 0.1% of our model a big deal, but might not immediately result in flipping one incorrect prediction to correct. But we want to be able to make small adjustments to our parameters. So we need a loss function that is smooth, meaning that small changes in the parameters will result in small changes in the loss. Smoothness also allows us to easily find gradients and derivatives.\nDraw the sigmoid function. What is special about its shape? Sigmoid takes all inputs and normalizes them into a value between 0 and 1. This allows easy optimization using SGD.\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nWhat is the difference between a loss function and a metric? A loss function has to be smooth, meaning that small changes in the parameters cause a response in the loss. Smoothness also allows us to easily find gradients and derivatives. A metric doesn‚Äôt have to be smooth. It just has to be a way of measuring how well the model is performing. A metric, on the other hand, is really what we care about. We care about accuracy, but this might not be amendable to optimization using SGD. As a human, we want to focus on this, rather than loss, in judging our model‚Äôs performance. The computer doing the optimizing, on the other hand, will use the loss to do its work. Interestingly loss is sometimes a compromise between two needs: our goal with the model and the ability of the function to be optimized using its gradient.\nWhat is the function to calculate new weights using a learning rate?\n\nnew_weight = old_weight - gradient*learning_rate\n\nWhat does the DataLoader class do? A DataLoader allows any python collection to be treated as an iterator. It has built in functionality to create batches and allows for shuffling, which improves the performance of training, as it gives our model variety. Notice below that listing the dl multiple times will return different batches.\n\nfrom torch.utils.data import DataLoader\n&gt;&gt;&gt; coll = range(2,26)\n&gt;&gt;&gt; dl = DataLoader(coll, batch_size=4, shuffle=True)\n&gt;&gt;&gt; list(dl)\n[tensor([13, 17, 23, 16]), tensor([ 4,  2, 12, 15]), tensor([ 5, 22, 10, 19]), tensor([ 7, 25, 21, 24]), tensor([ 6, 20,  8,  9]), tensor([ 3, 11, 18, 14])]\n&gt;&gt;&gt; list(dl)\n[tensor([17, 16, 22, 24]), tensor([21,  6, 12,  5]), tensor([14,  8,  4, 20]), tensor([15, 13, 18,  9]), tensor([11,  7, 23, 25]), tensor([19,  2,  3, 10])]\n&gt;&gt;&gt; list(dl)\n[tensor([12, 25, 24,  4]), tensor([ 5, 22,  2,  3]), tensor([13, 21, 23, 14]), tensor([10, 17,  6, 18]), tensor([15,  8, 19, 20]), tensor([11, 16,  7,  9])]\n\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n\nfor each opoch:\n  - for each batch\n    - make a prediction\n    - calculate the loss\n    - calculate the gradients\n    - updates the parameters based on the gradients * lr\n\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\ndef special(a, b):\n  return list(zip(a,b))\nThis structure is how pythorch expects to receive datasets. The first item in the tuple is the independant variable. The second item is the dependent variable. In other words these are the inputs and the targets of the model.\n\nWhat does view do in PyTorch? View allows pytorch to change the shape of a tensor without changing its contents. The new shape has to be compatible with the existing contents. For example a tensor([1,2,3,4]) can be reshaped into a tensor([[1,2],[3,4]]) because the number of elements is the same.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10))\n&gt;&gt;&gt; a\ntensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; a.view(3,3)\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\nWhat are the ‚Äúbias‚Äù parameters in a neural network? Why do we need them? The formula for a line is \\(y=w*x+b\\). Bias is the \\(b\\) in this formula. This allows our function to be more flexible, as in input of zero won‚Äôt always have to result in an output of zero.\nWhat does the @ operator do in Python? The @ operator performs matrix multiplication.\nWhat does the backward method do? The backward method calculates the gradients input parameters WRT the output of loss function. It does this by following the chain rule. It calculates the gradient of the loss function with respect to the output of the model, then the gradient of the output of the model with respect to the input parameters. It does this by following the computation graph that was created when the model was run. In order to be able to do this, the input params tensor need to have requires_grad set to True.\nWhy do we have to zero the gradients? We do this after we update the input parameters. This is because the gradients are accumulated. In other words, if we don‚Äôt zero them, they will continue to increase. We want to start fresh with each batch.\nWhat information do we have to pass to Learner? We have to pass the data, the model, the loss function, and the optimizer function (ie SGD), and optionally a metric function.\nShow Python or pseudocode for the basic steps of a training loop.\n\n- unpack our dl\n- calculate the loss, which implies making a prediciton with the model and comparing it to the actual label\n- calculate the gradients\n- update the parameters by param -= gradients * lr\n- zero the gradients\n\nWhat is ‚ÄúReLU‚Äù? Draw a plot of it for values from -2 to +2. A nonlinear function. For negative values its output is zero. For positive values its output is the input value.\nWhat is an ‚Äúactivation function‚Äù? These are the outputs of a layer in a nueral network.\nWhat‚Äôs the difference between F.relu and nn.ReLU? F.relu is a function. nn.ReLU is a class. They both do the same thing, but nn.ReLU is an object that can be used in a model.\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? We use more because it allows us to have fewer input parameters, which makese the model faster to train.\n\nextra credit üòè:\n\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter. kaggle notebook"
  },
  {
    "objectID": "posts/lesson 3/post.html#quiz",
    "href": "posts/lesson 3/post.html#quiz",
    "title": "FastAI Lesson 3",
    "section": "",
    "text": "How is a grayscale image represented on a computer? How about a color image? Images are represented as matrixes in a computer, with every cell in the matrix representing a pixel. In a grayscale image, the value of the cell is the intensity of the pixel. In a color image, the value of the cell is a vector of three values, representing the intensity of the red, green, and blue channels of the pixel.\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why? This dataset has a csv of labels and Train and ‚Äòvalid‚Äô folder. Within each folder, there is a subfolder for each label, eg /train/3. The data is organized this way because this is a common structure and allows for easy loading (dataLoader can read the parent folder and infer the labels from the subfolders).\nExplain how the ‚Äúpixel similarity‚Äù approach to classifying digits works. This approach approach takes the matrix of threes and calculates a mean matrix. We then compare a given image to this mean image. The comparison is performed using something a function that handles negative numbers but taking the absolute values or squaring the squaring the values. This is necessary because simple summing of the matrix resulting from taking the difference will cause some numbers to cancel each other out. The operation to deal with negative numbers (squaring or absolute value) is performed before calculating the mean. The result of this operation is a single number, which is the ‚Äúdistance‚Äù between the image and the mean image. The image is then classified as the label of the mean image with the smallest distance.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them. A list comprehension is a handy way of dealing with lists in python. It allows you to perform an operation on each element of a list and return a new list. For example, [x^2 for x in range(10) if x%2==1]will return a list of the odd numbers from 0 to 9, doubled.\nWhat is a ‚Äúrank-3 tensor‚Äù? A rank-3 tensor is a tensor with 3 dimensions. A rank-0 tensor is a single number, also called a scalar. A rank-1 tensor is a vector. A rank-2 tensor is a matrix. A rank-3 tensor is a 3-dimensional matrix, meaning a matrix of matrixes.\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape? Rank is the number of dimensions in a tensor. Shape is the size of each dimension. You can get the rank from the shape by counting the number of elements in the shape. For example Tensor([1,2,3,4]).shape will return torch.Size([4]) which has a rank 1.\n\n&gt;&gt;&gt; tensor([1],[1],[1]).shape\ntorch.Size([3, 1]) # rank 2, three rows, one column\n# stacked_threes is a a rank 3 tensor\n# it has shape torch.Size([6131, 28, 28]) \nstacked_threes[0][0][0].ndim # rank 0, ie scalar, ie raw value\nstacked_threes[0][0].ndim # rank 1, ie vector\nstacked_threes[0].ndim # rank 2, ie matrix\nstacked_threes.ndim # rank 3, ie 3-dimensional matrix\n\nWhat are RMSE and L1 norm? These are loss functions. RMSE is the root mean squared error. It is the square root of the mean of the squared differences between the predictions and the actual values. The L1 norm is the mean of the absolute value of the differences between the predictions and the actual values.\n\n# RMSE\ndef rmse(preds, targets):\n    return ((preds-targets)**2).mean().sqrt()\n# L1 norm\ndef l1(preds, targets):\n    return (preds-targets).abs().mean()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Using matrix multiplication. Matrix multiplication is a way of performing a calculation on a matrix of numbers. It is much faster than a python loop because it is implemented in C, which is much faster than python. The operation happens on the GPU, which is designed for this kind of operation.\nCreate a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10)).reshape(3,3)*2\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n&gt;&gt;&gt; a[1:,1:]\ntensor([[10, 12],\n        [16, 18]])\n\nWhat is broadcasting? Broadcasting is the ability of pytorch to perform operations on tensors of different shapes. For example, if you have a rank-1 tensor and a rank-2 tensor, pytorch will automatically expand the rank-1 tensor to match the shape of the rank-2 tensor. This is useful because it allows you to perform operations on tensors of different shapes without having to manually reshape them.\n\n&gt;&gt;&gt; a = torch.tensor([1,2,3])\n&gt;&gt;&gt; b = torch.tensor([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; a+b\ntensor([[2, 4, 6],\n        [5, 7, 9]])\n&gt;&gt;&gt; a.broadcast_to((2,3))\ntensor([[1, 2, 3],\n        [1, 2, 3]])\n&gt;&gt;&gt; b.broadcast_to((1,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]]])\n&gt;&gt;&gt; b.broadcast_to((2,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]],\n        [[1, 2, 3],\n         [4, 5, 6]]])\n\nAre metrics generally calculated using the training set, or the validation set? Why? Metrics are calculated using the validation set. We do this because we want to make sure the model is training to become generalized. If we used the training set, we would be optimizing the model to perform well on the training set, but we wouldn‚Äôt know if it was generalizing well.\nWhat is SGD? SGD is a way of optimizing a function, say for examples \\(y=x^2\\). SGD is a way of finding the minimum of this function. It does this by using the derivative to calculate the gradient, ie the slope at a particular point. The gradient tells us how changing the input parameters will change the output. If the gradient is positive, then increasing the input will increase the output. If the gradient is negative, then increasing the input will decrease the output. The gradient is calculated at a particular point, and then the input is adjusted by a small amount in the opposite direction of the gradient. This is repeated until the gradient is zero, which means that the input is at the minimum of the function. We are trying to minimize the function in our case because the function represents the loss of our model ie the gap between our model‚Äôs prediction and the actual value of the input.\nWhy does SGD use mini-batches? We use mini-batches in order to speed up the calculation in SGD. When we are optimizing, we are usually doing so over a large number of parameters. If we were to calculate the gradient for each parameter, it would take a long time. Instead, we can calculate the gradient for a small batch of parameters (as an average of the items in the batch), and then use that to adjust the parameters in the batch. This is much faster than calculating the gradient for each parameter individually.\nWhat are the seven steps in SGD for machine learning?\n\nInitialize the parameters with random values\nCalculate the predictions\nCalculate the loss\nCalculate the gradients, which approximates how the paramters need to change to reduce the loss\nAdjust the parameters by a small amount in the opposite direction of the gradient. ie If the gradient is positive, then decrease the parameter. If the gradient is negative, then increase the parameter.\nRepeat steps 2-5 until the loss is small enough for our purposes\n\nHow do we initialize the weights in a model? Randomly. That‚Äôs the ‚Äústochastic‚Äù part of SGD. We initialize the weights randomly because we don‚Äôt know what the optimal weights are. We are trying to find them. So we start with random weights and then use SGD to adjust them.\nWhat is ‚Äúloss‚Äù? Loss is a calculation of the difference between the model‚Äôs prediction and the actual value. It is a way of measuring how well the model is performing.\nWhy can‚Äôt we always use a high learning rate? If the learning rate is too high, we will bounce our parameters around, possible never arriving at the minimum. If the learning rate is too low, it will take a long time to arrive at the minimum.\nWhat is a ‚Äúgradient‚Äù? A gradient is a calculation of the slope of a function at a particular point. It tells us how changing the input parameters will change the output. Its the actual specific value of the derivative at a particular point in the loss function.\nDo you need to know how to calculate gradients yourself? Nope, pytorch does it for us.\nWhy can‚Äôt we use accuracy as a loss function? Accuracy is generally steppy. Meaning that an improvement of 0.1% of our model a big deal, but might not immediately result in flipping one incorrect prediction to correct. But we want to be able to make small adjustments to our parameters. So we need a loss function that is smooth, meaning that small changes in the parameters will result in small changes in the loss. Smoothness also allows us to easily find gradients and derivatives.\nDraw the sigmoid function. What is special about its shape? Sigmoid takes all inputs and normalizes them into a value between 0 and 1. This allows easy optimization using SGD.\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nWhat is the difference between a loss function and a metric? A loss function has to be smooth, meaning that small changes in the parameters cause a response in the loss. Smoothness also allows us to easily find gradients and derivatives. A metric doesn‚Äôt have to be smooth. It just has to be a way of measuring how well the model is performing. A metric, on the other hand, is really what we care about. We care about accuracy, but this might not be amendable to optimization using SGD. As a human, we want to focus on this, rather than loss, in judging our model‚Äôs performance. The computer doing the optimizing, on the other hand, will use the loss to do its work. Interestingly loss is sometimes a compromise between two needs: our goal with the model and the ability of the function to be optimized using its gradient.\nWhat is the function to calculate new weights using a learning rate?\n\nnew_weight = old_weight - gradient*learning_rate\n\nWhat does the DataLoader class do? A DataLoader allows any python collection to be treated as an iterator. It has built in functionality to create batches and allows for shuffling, which improves the performance of training, as it gives our model variety. Notice below that listing the dl multiple times will return different batches.\n\nfrom torch.utils.data import DataLoader\n&gt;&gt;&gt; coll = range(2,26)\n&gt;&gt;&gt; dl = DataLoader(coll, batch_size=4, shuffle=True)\n&gt;&gt;&gt; list(dl)\n[tensor([13, 17, 23, 16]), tensor([ 4,  2, 12, 15]), tensor([ 5, 22, 10, 19]), tensor([ 7, 25, 21, 24]), tensor([ 6, 20,  8,  9]), tensor([ 3, 11, 18, 14])]\n&gt;&gt;&gt; list(dl)\n[tensor([17, 16, 22, 24]), tensor([21,  6, 12,  5]), tensor([14,  8,  4, 20]), tensor([15, 13, 18,  9]), tensor([11,  7, 23, 25]), tensor([19,  2,  3, 10])]\n&gt;&gt;&gt; list(dl)\n[tensor([12, 25, 24,  4]), tensor([ 5, 22,  2,  3]), tensor([13, 21, 23, 14]), tensor([10, 17,  6, 18]), tensor([15,  8, 19, 20]), tensor([11, 16,  7,  9])]\n\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n\nfor each opoch:\n  - for each batch\n    - make a prediction\n    - calculate the loss\n    - calculate the gradients\n    - updates the parameters based on the gradients * lr\n\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\ndef special(a, b):\n  return list(zip(a,b))\nThis structure is how pythorch expects to receive datasets. The first item in the tuple is the independant variable. The second item is the dependent variable. In other words these are the inputs and the targets of the model.\n\nWhat does view do in PyTorch? View allows pytorch to change the shape of a tensor without changing its contents. The new shape has to be compatible with the existing contents. For example a tensor([1,2,3,4]) can be reshaped into a tensor([[1,2],[3,4]]) because the number of elements is the same.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10))\n&gt;&gt;&gt; a\ntensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; a.view(3,3)\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\nWhat are the ‚Äúbias‚Äù parameters in a neural network? Why do we need them? The formula for a line is \\(y=w*x+b\\). Bias is the \\(b\\) in this formula. This allows our function to be more flexible, as in input of zero won‚Äôt always have to result in an output of zero.\nWhat does the @ operator do in Python? The @ operator performs matrix multiplication.\nWhat does the backward method do? The backward method calculates the gradients input parameters WRT the output of loss function. It does this by following the chain rule. It calculates the gradient of the loss function with respect to the output of the model, then the gradient of the output of the model with respect to the input parameters. It does this by following the computation graph that was created when the model was run. In order to be able to do this, the input params tensor need to have requires_grad set to True.\nWhy do we have to zero the gradients? We do this after we update the input parameters. This is because the gradients are accumulated. In other words, if we don‚Äôt zero them, they will continue to increase. We want to start fresh with each batch.\nWhat information do we have to pass to Learner? We have to pass the data, the model, the loss function, and the optimizer function (ie SGD), and optionally a metric function.\nShow Python or pseudocode for the basic steps of a training loop.\n\n- unpack our dl\n- calculate the loss, which implies making a prediciton with the model and comparing it to the actual label\n- calculate the gradients\n- update the parameters by param -= gradients * lr\n- zero the gradients\n\nWhat is ‚ÄúReLU‚Äù? Draw a plot of it for values from -2 to +2. A nonlinear function. For negative values its output is zero. For positive values its output is the input value.\nWhat is an ‚Äúactivation function‚Äù? These are the outputs of a layer in a nueral network.\nWhat‚Äôs the difference between F.relu and nn.ReLU? F.relu is a function. nn.ReLU is a class. They both do the same thing, but nn.ReLU is an object that can be used in a model.\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? We use more because it allows us to have fewer input parameters, which makese the model faster to train.\n\nextra credit üòè:\n\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter. kaggle notebook"
  },
  {
    "objectID": "posts/lesson 7/post.html",
    "href": "posts/lesson 7/post.html",
    "title": "FastAI lesson 7",
    "section": "",
    "text": "These apex predators live over 70 years and grow as long as 23 feet.\n\n\n\n\n\nThis lesson was another lesson that covered a couple domains: vision and collaborative filtering. The vision part of the lesson worked through a couple more iterations of the rice paddy doctor competitions. First, jeremy demonstrated how to do gradient accumulution, showing how to write it into a training loop and then demonstrating the fastai callback that does it for you. Gradient accumulation is a technique that allows you to use a larger batch size than your GPU can handle by accumulating the gradients over multiple batches and then updating the weights after a certain number of batches. This allows you to use a larger batch size than you would otherwise be able to use, which can be useful for training on smaller datasets.\nHe then worked through how we would alter our model to predict multiple categories as an output. Beginning with noting that the model is not actually producing one output but 10 outputs that represent the probabilities of various diseases. He then coded the model by hand to have twenty activations and showed how we alter our loss and metric functions to account for the fact of multiple categories in the dataloader‚Äôs blocks parameter (1 input, 2 outputs). Given these multiple outputs, our loss and metrics callbacks now receive the outputs of the model in a tuple alongside the disease and rice variety labels. The loss function then calculates the loss for each output and returns the sum of the two losses. The metrics callback then calculates the error rate (1-accuracy, accuracy takes the mean of matching input to validations set as a float ).\nThe loss function itself is ‚Äúcross entropy loss‚Äù, which starts with using softmax to convert the activations into probabilities. Softmax is a function that takes a vector of numbers and converts them into a vector of probabilities that sum to 1. The cross entropy loss function then takes the softmax probabilities and compares them to the target labels, calculating the loss as the negative log of the probability of the correct label. The point of using the negative log is that it penalizes the model more for being more wrong.\nThe last part of the lesson looked at collaborative filtering using movie reviews and introduced this problem domain as well as the concept of encodings. Collaborative filtering is a technique for making recommendations based on the preferences of other users. The idea is that if you have a large enough dataset of user preferences, you can use that data to predict what a user might like based on what other users with similar preferences have liked. The first step in this process is to create a matrix of users and items (movies in this case) and then fill in the matrix with the ratings that each user has given to each movie. This matrix is called a utility matrix. The problem with this matrix is that it is sparse, meaning that most of the cells are empty. This is because most users have not rated most movies. The goal of collaborative filtering is to fill in the empty cells with predicted ratings.\nOne method for doing the more accurately (making a better model) is to use something called latent factors. Latent factors are a way of representing the preferences of users and the characteristics of items in a lower dimensional space aka an embedding. The idea is that you can represent the preferences of users and the characteristics of items in a lower dimensional space and then use that representation to predict the ratings of users for items. The way that you do this is by creating two matrices, one for users and one for items, and then multiplying them together to get a matrix of predicted ratings. The user matrix is a matrix of users and their preferences for each latent factor. The item matrix is a matrix of items and their characteristics for each latent factor. The predicted ratings matrix is the product of the user matrix and the item matrix. Finally, to improve our model further we want to add a bias to the predicted ratings. The bias is a number that is added to the predicted rating to account for the fact that some users tend to rate movies higher than others and some movies tend to be rated higher than others. In mathematical terms bias allows us to shift the predicted ratings up or down on the intercept. Finally jeremy added weight decay to the model to improve the model further. Weight decay is a technique for penalizing the model for having large weights. It does this by adding the sum of the squares of the weights to the loss function. This encourages the model to have smaller weights, which can help prevent overfitting. Essentially SGD is causing the model to balance the loss and the weight decay term.\n\n\n\n\nHow do you do gradient accumulation in fastai? You divide the loss by the number of accumulation steps in your training loop and then you pass a callback to Learner that accumulates the gradients, using the desired effective batch size as an argument.\nWhat is ensembling and how is it different with nueral networks vs random forests? Ensembling is the process of combining the predictions of multiple models to get a better prediction. With random forests, you can train multiple trees on different subsets of the data and then average their predictions. With neural networks, you can train multiple models on the same data and then average their predictions.\nWhat is TTA? TTA stands for test time augmentation. The idea is to do augmentation on the actual test input during inference and then average that. Augmentation in this case is doing the same thing that would happen during data/training augmentation (rotation, etc). This can make the model more robust to different inputs.\nWhat is k-fold cross validation? Is it better than doing straight ensembling? K-fold cross validation is a technique for evaluating a model by splitting the data into k folds and then training the model on k-1 folds and evaluating it on the remaining fold. This is repeated k times, each time using a different fold as the validation set. The results are then averaged. This is better than straight ensembling because it allows you to evaluate the model on more data.\nHow do you use it to get a better result from your test set? You can use it to get a better result from your test set by training your model on the entire dataset and then using k-fold cross validation to evaluate it. This allows you to use the entire dataset for training and testing.\nWhat is cross entropy loss? Cross entropy loss is a loss function that is used for classification problems. It is the negative log of the probability of the correct label. The probability is calculated using softmax.\nwhat is softmax? Softmax is a function that takes a vector of numbers and converts them into a vector of probabilities that sum to 1. It is used to convert the activations of the last layer of a neural network into probabilities.\nWhy can‚Äôt you use MSE as a loss function when trying to create a classifier? MSE works for loss in cases where the prediction is a quantitative output (for example predicting house prices). Its not ideal for classification problems because it doesn‚Äôt really make sense to think of probabilities as a quantitative output.\nWhat is binary cross entropy loss? What case is it for? This is a special case of cross entropy loss where the categories are binary (0 or 1).\nWhy is it that a multi-target model can have a higher accuracy than a single-target model, for the same data categorization task? Conceptually this can happen because the model learns more things about the problem space that might be relevant - for example in the case of rice diseases, the breed of rice may actually determine what diseases it is susceptible to. Mathematically, this can happen because the model is learning to predict multiple outputs, which means that it is learning more about the problem space.\nWhat is the last layer of most neural networks used for? What is are the activations of the last layer of a neural network? The last layer of most neural networks is used to convert the activations of the previous layer into the desired output. The activations of the last layer are the outputs of the model.\nWhat is an embedding and what is it used for? An embedding is a representation of a categorical variable in a lower dimensional space. It is used to represent the categorical variable in a way that is more useful for the model.\nWhat are latent factors? Why are they used? Latent factors are the factors that are used to represent the categorical variable in a lower dimensional space. They are used to represent the categorical variable in a way that is more useful for the model. They can be used without our explicit knowledge of what they are (which is amazing!)\nImplment a basic dot product model in PyTorch.\n\nclass DotProduct(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        super().__init__()\n        self.user_factors = nn.Embedding(n_users, n_factors)\n        self.movie_factors = nn.Embedding(n_movies, n_factors)\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.movie_bias = nn.Embedding(n_movies, 1)\n        \n    def forward(self, user, movie):\n        user_factors = self.user_factors(user)\n        movie_factors = self.movie_factors(movie)\n        user_bias = self.user_bias(user)\n        movie_bias = self.movie_bias(movie)\n        return (user_factors * movie_factors).sum(1, keepdim=True) + user_bias + movie_bias\n\nHow can you use a sigmoid to create a range of predictions? You can use a sigmoid to create a range of predictions by applying it to the output of the model. This will convert the output of the model into a probability between 0 and 1 (which can be scaled to a range of values).\nWhat is the use of the bias in a dot product model? Explain conceptually and mathematically. The bias in a dot product model is used to shift the predicted ratings up or down on the intercept. Conceptually, this is used to account for the fact that some users tend to rate movies higher than others and some movies tend to be rated higher than others (in this case). Mathematically, this is done by adding a bias term to the predicted ratings.\nWhat is another name for weight decay? What does weight decay do? Weight decay is also called L2 regularization. It penalizes the model for having large weights by adding the sum of the squares of the weights to the loss function. This encourages the model to have smaller weights, which can help prevent overfitting.\nWhy do we use it? What is the equation for it? We use weight decay to prevent overfitting. The equation for it is loss_with_weight_decay = loss + wd * (weights**2).sum(). Or in terms of the gradients, weight.grad += wd * 2 * weights (we can just use a larger or smaller value for wd and ignore the 2).\nWhat is weight decay balancing? Weight decay balancing is the process of finding the right value for weight decay. The idea is that you want to find a value that is large enough to prevent overfitting but not so large that it prevents the model from learning. Larger weights cause the model to be oversensitive to small changes in the input, which reduces its ability to generalize.\nWhat is regularization? Regularization is a technique for preventing overfitting. It is done by adding a term to the loss function that penalizes the model for having large weights. Basically this is another way of talking about weight decay. A model that is ‚Äúsimpler‚Äù is regularized and would have smaller weights."
  },
  {
    "objectID": "posts/lesson 7/post.html#tenacious-animal",
    "href": "posts/lesson 7/post.html#tenacious-animal",
    "title": "FastAI lesson 7",
    "section": "",
    "text": "These apex predators live over 70 years and grow as long as 23 feet."
  },
  {
    "objectID": "posts/lesson 7/post.html#recap",
    "href": "posts/lesson 7/post.html#recap",
    "title": "FastAI lesson 7",
    "section": "",
    "text": "This lesson was another lesson that covered a couple domains: vision and collaborative filtering. The vision part of the lesson worked through a couple more iterations of the rice paddy doctor competitions. First, jeremy demonstrated how to do gradient accumulution, showing how to write it into a training loop and then demonstrating the fastai callback that does it for you. Gradient accumulation is a technique that allows you to use a larger batch size than your GPU can handle by accumulating the gradients over multiple batches and then updating the weights after a certain number of batches. This allows you to use a larger batch size than you would otherwise be able to use, which can be useful for training on smaller datasets.\nHe then worked through how we would alter our model to predict multiple categories as an output. Beginning with noting that the model is not actually producing one output but 10 outputs that represent the probabilities of various diseases. He then coded the model by hand to have twenty activations and showed how we alter our loss and metric functions to account for the fact of multiple categories in the dataloader‚Äôs blocks parameter (1 input, 2 outputs). Given these multiple outputs, our loss and metrics callbacks now receive the outputs of the model in a tuple alongside the disease and rice variety labels. The loss function then calculates the loss for each output and returns the sum of the two losses. The metrics callback then calculates the error rate (1-accuracy, accuracy takes the mean of matching input to validations set as a float ).\nThe loss function itself is ‚Äúcross entropy loss‚Äù, which starts with using softmax to convert the activations into probabilities. Softmax is a function that takes a vector of numbers and converts them into a vector of probabilities that sum to 1. The cross entropy loss function then takes the softmax probabilities and compares them to the target labels, calculating the loss as the negative log of the probability of the correct label. The point of using the negative log is that it penalizes the model more for being more wrong.\nThe last part of the lesson looked at collaborative filtering using movie reviews and introduced this problem domain as well as the concept of encodings. Collaborative filtering is a technique for making recommendations based on the preferences of other users. The idea is that if you have a large enough dataset of user preferences, you can use that data to predict what a user might like based on what other users with similar preferences have liked. The first step in this process is to create a matrix of users and items (movies in this case) and then fill in the matrix with the ratings that each user has given to each movie. This matrix is called a utility matrix. The problem with this matrix is that it is sparse, meaning that most of the cells are empty. This is because most users have not rated most movies. The goal of collaborative filtering is to fill in the empty cells with predicted ratings.\nOne method for doing the more accurately (making a better model) is to use something called latent factors. Latent factors are a way of representing the preferences of users and the characteristics of items in a lower dimensional space aka an embedding. The idea is that you can represent the preferences of users and the characteristics of items in a lower dimensional space and then use that representation to predict the ratings of users for items. The way that you do this is by creating two matrices, one for users and one for items, and then multiplying them together to get a matrix of predicted ratings. The user matrix is a matrix of users and their preferences for each latent factor. The item matrix is a matrix of items and their characteristics for each latent factor. The predicted ratings matrix is the product of the user matrix and the item matrix. Finally, to improve our model further we want to add a bias to the predicted ratings. The bias is a number that is added to the predicted rating to account for the fact that some users tend to rate movies higher than others and some movies tend to be rated higher than others. In mathematical terms bias allows us to shift the predicted ratings up or down on the intercept. Finally jeremy added weight decay to the model to improve the model further. Weight decay is a technique for penalizing the model for having large weights. It does this by adding the sum of the squares of the weights to the loss function. This encourages the model to have smaller weights, which can help prevent overfitting. Essentially SGD is causing the model to balance the loss and the weight decay term."
  },
  {
    "objectID": "posts/lesson 7/post.html#quiz",
    "href": "posts/lesson 7/post.html#quiz",
    "title": "FastAI lesson 7",
    "section": "",
    "text": "How do you do gradient accumulation in fastai? You divide the loss by the number of accumulation steps in your training loop and then you pass a callback to Learner that accumulates the gradients, using the desired effective batch size as an argument.\nWhat is ensembling and how is it different with nueral networks vs random forests? Ensembling is the process of combining the predictions of multiple models to get a better prediction. With random forests, you can train multiple trees on different subsets of the data and then average their predictions. With neural networks, you can train multiple models on the same data and then average their predictions.\nWhat is TTA? TTA stands for test time augmentation. The idea is to do augmentation on the actual test input during inference and then average that. Augmentation in this case is doing the same thing that would happen during data/training augmentation (rotation, etc). This can make the model more robust to different inputs.\nWhat is k-fold cross validation? Is it better than doing straight ensembling? K-fold cross validation is a technique for evaluating a model by splitting the data into k folds and then training the model on k-1 folds and evaluating it on the remaining fold. This is repeated k times, each time using a different fold as the validation set. The results are then averaged. This is better than straight ensembling because it allows you to evaluate the model on more data.\nHow do you use it to get a better result from your test set? You can use it to get a better result from your test set by training your model on the entire dataset and then using k-fold cross validation to evaluate it. This allows you to use the entire dataset for training and testing.\nWhat is cross entropy loss? Cross entropy loss is a loss function that is used for classification problems. It is the negative log of the probability of the correct label. The probability is calculated using softmax.\nwhat is softmax? Softmax is a function that takes a vector of numbers and converts them into a vector of probabilities that sum to 1. It is used to convert the activations of the last layer of a neural network into probabilities.\nWhy can‚Äôt you use MSE as a loss function when trying to create a classifier? MSE works for loss in cases where the prediction is a quantitative output (for example predicting house prices). Its not ideal for classification problems because it doesn‚Äôt really make sense to think of probabilities as a quantitative output.\nWhat is binary cross entropy loss? What case is it for? This is a special case of cross entropy loss where the categories are binary (0 or 1).\nWhy is it that a multi-target model can have a higher accuracy than a single-target model, for the same data categorization task? Conceptually this can happen because the model learns more things about the problem space that might be relevant - for example in the case of rice diseases, the breed of rice may actually determine what diseases it is susceptible to. Mathematically, this can happen because the model is learning to predict multiple outputs, which means that it is learning more about the problem space.\nWhat is the last layer of most neural networks used for? What is are the activations of the last layer of a neural network? The last layer of most neural networks is used to convert the activations of the previous layer into the desired output. The activations of the last layer are the outputs of the model.\nWhat is an embedding and what is it used for? An embedding is a representation of a categorical variable in a lower dimensional space. It is used to represent the categorical variable in a way that is more useful for the model.\nWhat are latent factors? Why are they used? Latent factors are the factors that are used to represent the categorical variable in a lower dimensional space. They are used to represent the categorical variable in a way that is more useful for the model. They can be used without our explicit knowledge of what they are (which is amazing!)\nImplment a basic dot product model in PyTorch.\n\nclass DotProduct(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        super().__init__()\n        self.user_factors = nn.Embedding(n_users, n_factors)\n        self.movie_factors = nn.Embedding(n_movies, n_factors)\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.movie_bias = nn.Embedding(n_movies, 1)\n        \n    def forward(self, user, movie):\n        user_factors = self.user_factors(user)\n        movie_factors = self.movie_factors(movie)\n        user_bias = self.user_bias(user)\n        movie_bias = self.movie_bias(movie)\n        return (user_factors * movie_factors).sum(1, keepdim=True) + user_bias + movie_bias\n\nHow can you use a sigmoid to create a range of predictions? You can use a sigmoid to create a range of predictions by applying it to the output of the model. This will convert the output of the model into a probability between 0 and 1 (which can be scaled to a range of values).\nWhat is the use of the bias in a dot product model? Explain conceptually and mathematically. The bias in a dot product model is used to shift the predicted ratings up or down on the intercept. Conceptually, this is used to account for the fact that some users tend to rate movies higher than others and some movies tend to be rated higher than others (in this case). Mathematically, this is done by adding a bias term to the predicted ratings.\nWhat is another name for weight decay? What does weight decay do? Weight decay is also called L2 regularization. It penalizes the model for having large weights by adding the sum of the squares of the weights to the loss function. This encourages the model to have smaller weights, which can help prevent overfitting.\nWhy do we use it? What is the equation for it? We use weight decay to prevent overfitting. The equation for it is loss_with_weight_decay = loss + wd * (weights**2).sum(). Or in terms of the gradients, weight.grad += wd * 2 * weights (we can just use a larger or smaller value for wd and ignore the 2).\nWhat is weight decay balancing? Weight decay balancing is the process of finding the right value for weight decay. The idea is that you want to find a value that is large enough to prevent overfitting but not so large that it prevents the model from learning. Larger weights cause the model to be oversensitive to small changes in the input, which reduces its ability to generalize.\nWhat is regularization? Regularization is a technique for preventing overfitting. It is done by adding a term to the loss function that penalizes the model for having large weights. Basically this is another way of talking about weight decay. A model that is ‚Äúsimpler‚Äù is regularized and would have smaller weights."
  },
  {
    "objectID": "posts/learning observations/post.html",
    "href": "posts/learning observations/post.html",
    "title": "A Checkin on what‚Äôs working",
    "section": "",
    "text": "Observations on my learning process\n\nTenacious animal\n\n\n\nDall-e can‚Äôt spell\n\n\n\n\nThe learning process\nI thought it might be useful a this point to note a few things about my process of going through the course, so far. In addtiton to the content of the course, fastai is just brilliant in encorporating practices of learning that are miles more effective than the typical bottom up approach. There are also small ways the course does this, which are fascinating to try and pick out.\nAt the outset, knowing that my time was limited, it was important to me to optimize my learning. There are a number of resource that I drew on for this:\n\nBarbara Oakley‚Äôs course on learning how to learn\nMarcin, a fastai alums book on learning\nScott Young‚Äôs book on Ultralearning\nThe book ‚ÄúMake it Stick‚Äù\nThe book ‚ÄúThe First 20 Hours‚Äù\nThe paper ‚ÄúTeaching the Science of Learning‚Äù\n\nThat latter paper talks about 6 techniques that are effective for learning, which I‚Äôll elaborate on below:\n\nSpaced practice\nRetrieval practice\nElaboration\nInterleaving\nConcrete examples\nDual coding\n\nSpaced practice is relatively known (anki).\nRetrieval practice is suprising, in some ways. We learn by trying to retrieve information. And in fact actually quizzing yourself on information that you know nothing about somehow primes your brain to learn better. Jeremy has a number of structural elements in the course that are designed to do this. The quizzes at the end of each lesson are one example. The other is the ‚Äúfastai‚Äù library itself. It‚Äôs a library that is designed to be used by people who don‚Äôt know anything about deep learning.\nI‚Äôve been trying to use chatgpt to quiz me about the chapters‚Ä¶and its not great at specifics. In general (higher level) it gives good questions. Quizzing is a big part of it. I know try and go through the quizzes before, during, and after lessons. I also try and do the clean notebooks, quzzing myeslef on the outputs. Another technique I haven‚Äôt tried, but would like to, is doing a sort of ‚Äúflipped‚Äù approach to the clean notebooks, where I try and create the code based on the prose descriptions.\nInterleaving is a technique I haven‚Äôt tried. Basically you weave subjects together. Like I could weave vision and tabular DL learning practice.\nConcrete examples are obvious, and are how the course is rooted.\nDual coding is about presenting the information in multiple forms, like visual and written. It isn‚Äôt about learning styles, but about complementary information.\nI‚Äôve also been experimenting with using various LLM tools to both enhance my learning and my coding productivity. For example the learner I developed for the last lesson used copilot and chatgpt to help create the classes, do some refactoring, and some debugging. I‚Äôve also been trying to use a remote jupyter kernel with a local notebook instance so I can utilize copilot as part of my authoring process. I think I‚Äôll be experimenting with nbdev next.\nAnyway this is a quick dump of some techniques and experiments I‚Äôve been trying. I‚Äôll write later to see how they are working."
  }
]