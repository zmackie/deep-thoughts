[
  {
    "objectID": "posts/week2/post.html",
    "href": "posts/week2/post.html",
    "title": "FastAI Lesson 2",
    "section": "",
    "text": "This lesson was all about getting a model deployed to production. It was an occasion for me to make sure I had jupyter working locally and to understand how that flow would works. I used Jeremy‚Äôs technique of building an app in notebook and exporting it with nbdev directives, which is super neat! I also set myself up on paperspace, which I‚Äôd highly endorese. Eight dollars well spent.\nThis lesson‚Äôs clean notebook was basically a recap of lesson 1, with bears instead of cats and dogs or birds.\nI deployed my model to a Huggingface Space using Gradio. This is based on a model I trained last week for recognizing bean lesions. So if you happen to be a farmer wanting to evaluated the health of your beans‚Ä¶you know who to call üò∏. I skipped the step of using the api for this app. Gradio now has an API client that has to be installed with npm. Which‚Ä¶Im not trying to deploy anything that needs a build step. I wish they hadn‚Äôt done that.\nI hit a few hiccups which were easily navigated with some googling. Mainly these were API changes in various libraries. Inevitable as things move along but bumps nonetheless.\n\n\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. Not great at images that differ in stylisitically. The camera to recognize the bears might be mounted upside down.\nWhere do text models currently have a major deficiency? They‚Äôre plausible, but are they correct?!\nWhat are possible negative societal implications of text generation models? Spreading false information that‚Äôs very convincing\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Human in the loop.\nWhat kind of tabular data is deep learning particularly good at? where the column data might be very diverse\nWhat‚Äôs a key downside of directly using a deep learning model for recommendation systems? The recommendations might not actually be helpful. eg. they might recommend books you already have\nWhat are the steps of the Drivetrain Approach? objectives, levers, data, model (how the levers influence the objectives)\nHow do the steps of the Drivetrain Approach map to a recommendation system? objective: drive more sales levers: ranking of the recommendations data: past choices of the user model: two models that are contingent on seeing or not seeing the recommendation\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders? Generic class for getting data into a learner\nWhat four things do we need to tell fastai to create DataLoaders?\ninput and output types\nhow get get the items\nhow to label the items\nhow to create a validaton set\nWhat does the splitter parameter to DataBlock do? telling fai how to split of a validation set\nHow do we ensure a random split always gives the same validation set? seed will set the randomness\nWhat letters are often used to signify the independent and dependent variables? x for independent y for dependent\nWhat‚Äôs the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? crop = cut pad = fill in with black squish = distort to fit chose might be dictated by the particulars of the data\nWhat is data augmentation? Why is it needed? creating random variations that seem different (to a model) but don‚Äôt actually change in meaning\nWhat is the difference between item_tfms and batch_tfms? item is single, batch is a group\nWhat is a confusion matrix? a nXn matrix that plots what the model was predicted vs the correct labels. the center diagonal is correct guess. others a re off somehow. lets us see issues in data or model. uses the validation set.\nWhat does export save? a pkl file of the trained model\nWhat is it called when we use a model for getting predictions, instead of training? inference\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better? cpu is more cost effective, easier to manage, more available, and perfectly suitable for running inferences.\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? network issues, latency, sensitive data, complexity of runnning infra\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? data is video vs pictures, bears in are in novel positions or lighting, night pictures, speed of results\nWhat is ‚Äúout-of-domain data‚Äù? data that differs a lot to what was seen in training\nWhat is ‚Äúdomain shift‚Äù? type of data changes over time so the initial model doesnt apply so much. the use of the model actually changes things, so the model has to be adjusted.\nWhat are the three steps in the deployment process?\nmanual steps, human checks it all\nlimited scope. time or geography limited. careful supervisions\ngradual expansion. need reporting. need to consider what can go wrong."
  },
  {
    "objectID": "posts/week2/post.html#quiz",
    "href": "posts/week2/post.html#quiz",
    "title": "FastAI Lesson 2",
    "section": "",
    "text": "Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. Not great at images that differ in stylisitically. The camera to recognize the bears might be mounted upside down.\nWhere do text models currently have a major deficiency? They‚Äôre plausible, but are they correct?!\nWhat are possible negative societal implications of text generation models? Spreading false information that‚Äôs very convincing\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Human in the loop.\nWhat kind of tabular data is deep learning particularly good at? where the column data might be very diverse\nWhat‚Äôs a key downside of directly using a deep learning model for recommendation systems? The recommendations might not actually be helpful. eg. they might recommend books you already have\nWhat are the steps of the Drivetrain Approach? objectives, levers, data, model (how the levers influence the objectives)\nHow do the steps of the Drivetrain Approach map to a recommendation system? objective: drive more sales levers: ranking of the recommendations data: past choices of the user model: two models that are contingent on seeing or not seeing the recommendation\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders? Generic class for getting data into a learner\nWhat four things do we need to tell fastai to create DataLoaders?\ninput and output types\nhow get get the items\nhow to label the items\nhow to create a validaton set\nWhat does the splitter parameter to DataBlock do? telling fai how to split of a validation set\nHow do we ensure a random split always gives the same validation set? seed will set the randomness\nWhat letters are often used to signify the independent and dependent variables? x for independent y for dependent\nWhat‚Äôs the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? crop = cut pad = fill in with black squish = distort to fit chose might be dictated by the particulars of the data\nWhat is data augmentation? Why is it needed? creating random variations that seem different (to a model) but don‚Äôt actually change in meaning\nWhat is the difference between item_tfms and batch_tfms? item is single, batch is a group\nWhat is a confusion matrix? a nXn matrix that plots what the model was predicted vs the correct labels. the center diagonal is correct guess. others a re off somehow. lets us see issues in data or model. uses the validation set.\nWhat does export save? a pkl file of the trained model\nWhat is it called when we use a model for getting predictions, instead of training? inference\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better? cpu is more cost effective, easier to manage, more available, and perfectly suitable for running inferences.\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? network issues, latency, sensitive data, complexity of runnning infra\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? data is video vs pictures, bears in are in novel positions or lighting, night pictures, speed of results\nWhat is ‚Äúout-of-domain data‚Äù? data that differs a lot to what was seen in training\nWhat is ‚Äúdomain shift‚Äù? type of data changes over time so the initial model doesnt apply so much. the use of the model actually changes things, so the model has to be adjusted.\nWhat are the three steps in the deployment process?\nmanual steps, human checks it all\nlimited scope. time or geography limited. careful supervisions\ngradual expansion. need reporting. need to consider what can go wrong."
  },
  {
    "objectID": "posts/lesson 4/post.html",
    "href": "posts/lesson 4/post.html",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "Dung beetles can lift 1141 times their own body weight.\n\n\n\n\n\nThis lesson was about NLP. The video differs from the book, in that it uses the huggingface (ü§ó) transformers library. This library provides a convenient mid-level api for working with models. In the lesson there were a couple salient point: - Using a language model for classification (or sequence classification or sentiment analysis). Basically this is about outputting a category label given an input document. - Thinking about how to turn things into classification problems. The basic instance of this type of problem would be sentiment analysis (ie ‚Äòis this review favorable or not?‚Äô). In the case of the example data, we‚Äôre trying to label patent categories&lt;&gt;descriptions tuples as being more or less similar. - Constructing appropriate training, validation, and test sets. - Using pandas dataframes. - Pearson coefficient for calculating metrics. We use this because that‚Äôs how kaggle calculates the contest. - The tokenization process and how that all works (basically splitting up the input document into a list of tokens and then turning those tokens into numbers). This answers the question of how a mathematical function can operate on a document.\nI played with reproducing the results using a different model, distilbert-base-uncased, which is the ü§ó default model. It took a while to massage our previous metrics function to work. They have an evaluate library which I used. My results were worse and took longer to train, but it was useful to go through the exercise. I played with a few things like feature engineering (creating the input documents in various ways) and training for longer, but I was never able to match the results. I also played with using a specifically patent trained language model for this. That work is in process because it involves converting a AutoModelForSeq2SeqLM model into a classifier model. That‚Äôs a project I want to understand a little more deeply and will write about when I get it to work. Chatgpt gave me some help, but I think I‚Äôll experiment with copilot to see what it can do. It seems to involve adding a classification layer which takes the outputs of the model and reduces them down to N categories.\nHere‚Äôs what I‚Äôve got so far. Its not working yet üòè:\n\nKaggle Notebook\n\n\n\n\n\nWhat is ‚Äúself-supervised learning‚Äù? Learning where there are no labels, per-se\nWhat is a ‚Äúlanguage model‚Äù? A language model is a model that predicts the next word in a sequence of words.\nWhy is a language model considered self-supervised? Because it is trained on a corpus of text, and the labels are the next word in the sequence. This means the labels are the input text, just shifted by one word. We don‚Äôt have to explicitey provide labels.\nWhat are self-supervised models usually used for? They are used to create a representation of the input data that can be used for other tasks.\nWhy do we fine-tune language models? Because the language model is trained on a corpus of text that is different from the corpus of text we want to use it on. So we fine-tune it on the corpus of text we want to use it on.\nWhat are the three steps to create a state-of-the-art text classifier?\n\nFine-tune a language model on a dataset of unlabeled text\nFine-tune the language model on your labeled dataset\nUse the fine-tuned language model to train a classifier\n\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? This gives the language model context specific knowledge about the language used in movie reviews.\nWhat are the three steps to prepare your data for a language model?\n\nTokenize the text\nNumericalize the tokens\nCreate batches\n\nWhat is ‚Äútokenization‚Äù? Why do we need it? Tokenization is the process of splitting a document into a list of tokens. We need it because we can‚Äôt feed text into a neural network, we need to feed numbers.\nName three different approaches to tokenization.\n\nSplit on spaces\nSplit on characters\nUse a library like spaCy\n\nWhat is xxbos? It is a token that indicates the beginning of a text.\nList four rules that fastai applies to text during tokenization.\n\nAll text is lowercased\nAll punctuation is replaced with tokens\nAll numbers are replaced with a special token\nWords are replaced with a token if they are not in the vocabulary\n\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that‚Äôs repeated? Because it is a common pattern in text, and it is useful to have a token for it. It also compacts the vocabulary.\nWhat is ‚Äúnumericalization‚Äù? Numericalization is the process of mapping tokens to integers.\nWhy might there be words that are replaced with the ‚Äúunknown word‚Äù token? Because they are not in the vocabulary.\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book‚Äôs website.) The second row of the tensor contains the second batch. The first row of the second batch contains the 65th to 128th tokens.\nWhy do we need padding for text classification? Why don‚Äôt we need it for language modeling? We need padding for text classification because we need to have the same length for each input. We don‚Äôt need it for language modeling because we are predicting the next word in the sequence, so we don‚Äôt need to have the same length for each input.\nWhat does an embedding matrix for NLP contain? What is its shape? An embedding matrix contains the embeddings for each token in the vocabulary. It has a shape of (vocab_size, embedding_size).\nWhat is ‚Äúperplexity‚Äù? Perplexity is a measure of how well a probability distribution or probability model predicts a sample.\nWhy do we have to pass the vocabulary of the language model to the classifier data block? Because we want to use the same vocabulary for the classifier as we used for the language model.\nWhat is ‚Äúgradual unfreezing‚Äù? Gradual unfreezing is the process of unfreezing one layer at a time and training the model.\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts? Because generation can be trained using a classifier, but identification can‚Äôt be trained using a generator."
  },
  {
    "objectID": "posts/lesson 4/post.html#tenacious-animal",
    "href": "posts/lesson 4/post.html#tenacious-animal",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "Dung beetles can lift 1141 times their own body weight."
  },
  {
    "objectID": "posts/lesson 4/post.html#recap",
    "href": "posts/lesson 4/post.html#recap",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "This lesson was about NLP. The video differs from the book, in that it uses the huggingface (ü§ó) transformers library. This library provides a convenient mid-level api for working with models. In the lesson there were a couple salient point: - Using a language model for classification (or sequence classification or sentiment analysis). Basically this is about outputting a category label given an input document. - Thinking about how to turn things into classification problems. The basic instance of this type of problem would be sentiment analysis (ie ‚Äòis this review favorable or not?‚Äô). In the case of the example data, we‚Äôre trying to label patent categories&lt;&gt;descriptions tuples as being more or less similar. - Constructing appropriate training, validation, and test sets. - Using pandas dataframes. - Pearson coefficient for calculating metrics. We use this because that‚Äôs how kaggle calculates the contest. - The tokenization process and how that all works (basically splitting up the input document into a list of tokens and then turning those tokens into numbers). This answers the question of how a mathematical function can operate on a document.\nI played with reproducing the results using a different model, distilbert-base-uncased, which is the ü§ó default model. It took a while to massage our previous metrics function to work. They have an evaluate library which I used. My results were worse and took longer to train, but it was useful to go through the exercise. I played with a few things like feature engineering (creating the input documents in various ways) and training for longer, but I was never able to match the results. I also played with using a specifically patent trained language model for this. That work is in process because it involves converting a AutoModelForSeq2SeqLM model into a classifier model. That‚Äôs a project I want to understand a little more deeply and will write about when I get it to work. Chatgpt gave me some help, but I think I‚Äôll experiment with copilot to see what it can do. It seems to involve adding a classification layer which takes the outputs of the model and reduces them down to N categories.\nHere‚Äôs what I‚Äôve got so far. Its not working yet üòè:\n\nKaggle Notebook"
  },
  {
    "objectID": "posts/lesson 4/post.html#quiz",
    "href": "posts/lesson 4/post.html#quiz",
    "title": "FastAI Lesson 4",
    "section": "",
    "text": "What is ‚Äúself-supervised learning‚Äù? Learning where there are no labels, per-se\nWhat is a ‚Äúlanguage model‚Äù? A language model is a model that predicts the next word in a sequence of words.\nWhy is a language model considered self-supervised? Because it is trained on a corpus of text, and the labels are the next word in the sequence. This means the labels are the input text, just shifted by one word. We don‚Äôt have to explicitey provide labels.\nWhat are self-supervised models usually used for? They are used to create a representation of the input data that can be used for other tasks.\nWhy do we fine-tune language models? Because the language model is trained on a corpus of text that is different from the corpus of text we want to use it on. So we fine-tune it on the corpus of text we want to use it on.\nWhat are the three steps to create a state-of-the-art text classifier?\n\nFine-tune a language model on a dataset of unlabeled text\nFine-tune the language model on your labeled dataset\nUse the fine-tuned language model to train a classifier\n\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? This gives the language model context specific knowledge about the language used in movie reviews.\nWhat are the three steps to prepare your data for a language model?\n\nTokenize the text\nNumericalize the tokens\nCreate batches\n\nWhat is ‚Äútokenization‚Äù? Why do we need it? Tokenization is the process of splitting a document into a list of tokens. We need it because we can‚Äôt feed text into a neural network, we need to feed numbers.\nName three different approaches to tokenization.\n\nSplit on spaces\nSplit on characters\nUse a library like spaCy\n\nWhat is xxbos? It is a token that indicates the beginning of a text.\nList four rules that fastai applies to text during tokenization.\n\nAll text is lowercased\nAll punctuation is replaced with tokens\nAll numbers are replaced with a special token\nWords are replaced with a token if they are not in the vocabulary\n\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that‚Äôs repeated? Because it is a common pattern in text, and it is useful to have a token for it. It also compacts the vocabulary.\nWhat is ‚Äúnumericalization‚Äù? Numericalization is the process of mapping tokens to integers.\nWhy might there be words that are replaced with the ‚Äúunknown word‚Äù token? Because they are not in the vocabulary.\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book‚Äôs website.) The second row of the tensor contains the second batch. The first row of the second batch contains the 65th to 128th tokens.\nWhy do we need padding for text classification? Why don‚Äôt we need it for language modeling? We need padding for text classification because we need to have the same length for each input. We don‚Äôt need it for language modeling because we are predicting the next word in the sequence, so we don‚Äôt need to have the same length for each input.\nWhat does an embedding matrix for NLP contain? What is its shape? An embedding matrix contains the embeddings for each token in the vocabulary. It has a shape of (vocab_size, embedding_size).\nWhat is ‚Äúperplexity‚Äù? Perplexity is a measure of how well a probability distribution or probability model predicts a sample.\nWhy do we have to pass the vocabulary of the language model to the classifier data block? Because we want to use the same vocabulary for the classifier as we used for the language model.\nWhat is ‚Äúgradual unfreezing‚Äù? Gradual unfreezing is the process of unfreezing one layer at a time and training the model.\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts? Because generation can be trained using a classifier, but identification can‚Äôt be trained using a generator."
  },
  {
    "objectID": "posts/lesson 3/post.html",
    "href": "posts/lesson 3/post.html",
    "title": "FastAI Lesson 3",
    "section": "",
    "text": "This was a tough lesson. I started calling this the ‚Äúweek 3 wall‚Äù because I felt just totally stopped in my tracks by this one. Conceptually it just took me a a long time to wrap my head around this one. One of the things I‚Äôve had to embrace with this course to make progress is just allowing myself to not understand all the details of something. Like for example I get the idea of using a derivative to optimize a quadratic function. Somehow intuitively I can make sense of the image of a tangent line moving down a function and slowly flattening out at the bottom of the graph. But putting all the pieces together has been a struggle. I worked through the titanic example on my own, and that helped it make a bit more sense. What helped there was going from the linear function to the super simple two-layer nueral net. And applying matrix multiplication there was helpful to cement the concept.\nAs Jeremy has said time and again, tenacity is super important. I wasn‚Äôt going to let this week stall me. To inspire myself I generated some fun pictures in Dall-e of especially tenacious animals. Here‚Äôs a honey-badger for you viewing pleasure. I encourage you to adopt it as your fastai spirit animal.\n\n\n\nOne of the crucial things that took me a long time to understand is the relationship between the loss function and the model. For some reason I had the idea that the loss function was related to the model somehow, like in some kind of mathematical sense. But the crucial thing here to understand is that the model (which includes the architecture and the parameters) is tangential to the loss function in SGD. The loss function is a way to understand the performance of the model WRT a known answer. SGD is a method of using the loss function (which is somewhat arbitrary - at least that‚Äôs what it seems like a the moment) to optimize or train the values of the weights of the model. The ‚Äúgradient‚Äù is actually a calculation of the slope of the loss function WRT the input parameters, and gradients might be many gradients - one for each parameter. The gradient is calculated using the derivative of the loss function, which says for a change in the input, how will the loss be changed. What is mysterious to me is the backwards/back propogation step. What‚Äôs ultimately confusing to me is the fact that the loss function helps the input parameters adjust, but the input parameters are not directly involved in the loss function at all! They loss function only uses the predictions and the known answers. So how does the loss function help the parameters adjust? I think this is the backwards/back propogation step, but I‚Äôm not sure. I think I need to do some more reading on this. The key seems to be the ‚Äúchain rule‚Äù.\n\nThe chain rule is a fundamental principle in calculus that states how to compute the derivative of a composite function. Concisely, it can be stated as:\n\n\n‚ÄúIf a function \\(( y )\\) is a composite of another function \\((u)\\), such that \\((y=f(u))\\) and \\((u=g(x))\\), then the derivative of \\(( y )\\) with respect to \\(( x )\\) is the product of the derivative of ( f ) with respect to ( u ) and the derivative of ( g ) with respect to ( x ). In mathematical terms, it is expressed as \\(( \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} )\\).‚Äù\n\nConceptually I can start to grasp that there is function composition going on, but the details are still a bit fuzzy. Ultimately the way I‚Äôm thinking about it is that the prediction is itself the output of a function. But thinking about the prediction as an arbitary set of values isn‚Äôt quite correct. Because I started thinking, well if the prediction is just an array of values, how is it possible to know anything about what produced those values? And in the general sense its not. But this is what pytorch.requires_grad allows us to do. Setting that parameter on the input actually causes the library to records a computation graph and the prediction (or the output of running the params through that graph) will be statefully associated with the input parameters via that graph, which is what allows us to call .backward() on the prediction and have the gradients calculated. Because the prediction has knowledge of what produced it, its possible to then follow that chain of operations backwards, calculating the gradients of each step, and then using those gradients to adjust the parameters. I think this is the key to understanding the backwards/back propogation step.\nGetting the breed recognizer model deployed was a bit trickier than the last model. Due to the use of some of the models from timm, I entered into some fiddly dependency dancing. Luckily other people had figured this out for me and so a quick google (which turned up an answer on the fastai forums) turned up a fix. One of the weirder things that I‚Äôm struggling with in terms of deployment is how to manage dependencies for an app built in this way. Running the app from jupyterhub and running a pip install puts the packages into the conda environment that jupyterhub is running in. This makes it slightly hard to understand the dependencies and respective versions that need to go into the requirements.txt file. Ultimately I had to use --report which gives an output of package versions and then use that to populate the requirements.txt file. I‚Äôm not sure if this is the best way to do this, but it‚Äôs what I‚Äôve got for now. Probably there‚Äôs a a better way to use poetry or venv or something to manage this.\nI also found it valuable to run through pytorchs basic tensor tutorial to wrap my head around this datatype a bit more. I found myself reading the torch/tensor Docs a lot, and noticed that the torch.method() docs usually have examples, which makese it easier to understand things. However playing with the stuff in my terminal was always the most illuminating.\n&gt;&gt;&gt; t = torch.randint(high=10,size=(2,3))\n&gt;&gt;&gt; t\ntensor([[8, 0, 7],\n        [0, 7, 3]])\n&gt;&gt;&gt; t.T\ntensor([[8, 0],\n        [0, 7],\n        [7, 3]])\n&gt;&gt;&gt; t.T.shape\ntorch.Size([3, 2])\n&gt;&gt;&gt; t.shape\ntorch.Size([2, 3])\nIts still totally magical to me, though, that I can actually make these things!\n\n\n\nHow is a grayscale image represented on a computer? How about a color image? Images are represented as matrixes in a computer, with every cell in the matrix representing a pixel. In a grayscale image, the value of the cell is the intensity of the pixel. In a color image, the value of the cell is a vector of three values, representing the intensity of the red, green, and blue channels of the pixel.\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why? This dataset has a csv of labels and Train and ‚Äòvalid‚Äô folder. Within each folder, there is a subfolder for each label, eg /train/3. The data is organized this way because this is a common structure and allows for easy loading (dataLoader can read the parent folder and infer the labels from the subfolders).\nExplain how the ‚Äúpixel similarity‚Äù approach to classifying digits works. This approach approach takes the matrix of threes and calculates a mean matrix. We then compare a given image to this mean image. The comparison is performed using something a function that handles negative numbers but taking the absolute values or squaring the squaring the values. This is necessary because simple summing of the matrix resulting from taking the difference will cause some numbers to cancel each other out. The operation to deal with negative numbers (squaring or absolute value) is performed before calculating the mean. The result of this operation is a single number, which is the ‚Äúdistance‚Äù between the image and the mean image. The image is then classified as the label of the mean image with the smallest distance.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them. A list comprehension is a handy way of dealing with lists in python. It allows you to perform an operation on each element of a list and return a new list. For example, [x^2 for x in range(10) if x%2==1]will return a list of the odd numbers from 0 to 9, doubled.\nWhat is a ‚Äúrank-3 tensor‚Äù? A rank-3 tensor is a tensor with 3 dimensions. A rank-0 tensor is a single number, also called a scalar. A rank-1 tensor is a vector. A rank-2 tensor is a matrix. A rank-3 tensor is a 3-dimensional matrix, meaning a matrix of matrixes.\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape? Rank is the number of dimensions in a tensor. Shape is the size of each dimension. You can get the rank from the shape by counting the number of elements in the shape. For example Tensor([1,2,3,4]).shape will return torch.Size([4]) which has a rank 1.\n\n&gt;&gt;&gt; tensor([1],[1],[1]).shape\ntorch.Size([3, 1]) # rank 2, three rows, one column\n# stacked_threes is a a rank 3 tensor\n# it has shape torch.Size([6131, 28, 28]) \nstacked_threes[0][0][0].ndim # rank 0, ie scalar, ie raw value\nstacked_threes[0][0].ndim # rank 1, ie vector\nstacked_threes[0].ndim # rank 2, ie matrix\nstacked_threes.ndim # rank 3, ie 3-dimensional matrix\n\nWhat are RMSE and L1 norm? These are loss functions. RMSE is the root mean squared error. It is the square root of the mean of the squared differences between the predictions and the actual values. The L1 norm is the mean of the absolute value of the differences between the predictions and the actual values.\n\n# RMSE\ndef rmse(preds, targets):\n    return ((preds-targets)**2).mean().sqrt()\n# L1 norm\ndef l1(preds, targets):\n    return (preds-targets).abs().mean()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Using matrix multiplication. Matrix multiplication is a way of performing a calculation on a matrix of numbers. It is much faster than a python loop because it is implemented in C, which is much faster than python. The operation happens on the GPU, which is designed for this kind of operation.\nCreate a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10)).reshape(3,3)*2\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n&gt;&gt;&gt; a[1:,1:]\ntensor([[10, 12],\n        [16, 18]])\n\nWhat is broadcasting? Broadcasting is the ability of pytorch to perform operations on tensors of different shapes. For example, if you have a rank-1 tensor and a rank-2 tensor, pytorch will automatically expand the rank-1 tensor to match the shape of the rank-2 tensor. This is useful because it allows you to perform operations on tensors of different shapes without having to manually reshape them.\n\n&gt;&gt;&gt; a = torch.tensor([1,2,3])\n&gt;&gt;&gt; b = torch.tensor([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; a+b\ntensor([[2, 4, 6],\n        [5, 7, 9]])\n&gt;&gt;&gt; a.broadcast_to((2,3))\ntensor([[1, 2, 3],\n        [1, 2, 3]])\n&gt;&gt;&gt; b.broadcast_to((1,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]]])\n&gt;&gt;&gt; b.broadcast_to((2,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]],\n        [[1, 2, 3],\n         [4, 5, 6]]])\n\nAre metrics generally calculated using the training set, or the validation set? Why? Metrics are calculated using the validation set. We do this because we want to make sure the model is training to become generalized. If we used the training set, we would be optimizing the model to perform well on the training set, but we wouldn‚Äôt know if it was generalizing well.\nWhat is SGD? SGD is a way of optimizing a function, say for examples \\(y=x^2\\). SGD is a way of finding the minimum of this function. It does this by using the derivative to calculate the gradient, ie the slope at a particular point. The gradient tells us how changing the input parameters will change the output. If the gradient is positive, then increasing the input will increase the output. If the gradient is negative, then increasing the input will decrease the output. The gradient is calculated at a particular point, and then the input is adjusted by a small amount in the opposite direction of the gradient. This is repeated until the gradient is zero, which means that the input is at the minimum of the function. We are trying to minimize the function in our case because the function represents the loss of our model ie the gap between our model‚Äôs prediction and the actual value of the input.\nWhy does SGD use mini-batches? We use mini-batches in order to speed up the calculation in SGD. When we are optimizing, we are usually doing so over a large number of parameters. If we were to calculate the gradient for each parameter, it would take a long time. Instead, we can calculate the gradient for a small batch of parameters (as an average of the items in the batch), and then use that to adjust the parameters in the batch. This is much faster than calculating the gradient for each parameter individually.\nWhat are the seven steps in SGD for machine learning?\n\nInitialize the parameters with random values\nCalculate the predictions\nCalculate the loss\nCalculate the gradients, which approximates how the paramters need to change to reduce the loss\nAdjust the parameters by a small amount in the opposite direction of the gradient. ie If the gradient is positive, then decrease the parameter. If the gradient is negative, then increase the parameter.\nRepeat steps 2-5 until the loss is small enough for our purposes\n\nHow do we initialize the weights in a model? Randomly. That‚Äôs the ‚Äústochastic‚Äù part of SGD. We initialize the weights randomly because we don‚Äôt know what the optimal weights are. We are trying to find them. So we start with random weights and then use SGD to adjust them.\nWhat is ‚Äúloss‚Äù? Loss is a calculation of the difference between the model‚Äôs prediction and the actual value. It is a way of measuring how well the model is performing.\nWhy can‚Äôt we always use a high learning rate? If the learning rate is too high, we will bounce our parameters around, possible never arriving at the minimum. If the learning rate is too low, it will take a long time to arrive at the minimum.\nWhat is a ‚Äúgradient‚Äù? A gradient is a calculation of the slope of a function at a particular point. It tells us how changing the input parameters will change the output. Its the actual specific value of the derivative at a particular point in the loss function.\nDo you need to know how to calculate gradients yourself? Nope, pytorch does it for us.\nWhy can‚Äôt we use accuracy as a loss function? Accuracy is generally steppy. Meaning that an improvement of 0.1% of our model a big deal, but might not immediately result in flipping one incorrect prediction to correct. But we want to be able to make small adjustments to our parameters. So we need a loss function that is smooth, meaning that small changes in the parameters will result in small changes in the loss. Smoothness also allows us to easily find gradients and derivatives.\nDraw the sigmoid function. What is special about its shape? Sigmoid takes all inputs and normalizes them into a value between 0 and 1. This allows easy optimization using SGD.\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nWhat is the difference between a loss function and a metric? A loss function has to be smooth, meaning that small changes in the parameters cause a response in the loss. Smoothness also allows us to easily find gradients and derivatives. A metric doesn‚Äôt have to be smooth. It just has to be a way of measuring how well the model is performing. A metric, on the other hand, is really what we care about. We care about accuracy, but this might not be amendable to optimization using SGD. As a human, we want to focus on this, rather than loss, in judging our model‚Äôs performance. The computer doing the optimizing, on the other hand, will use the loss to do its work. Interestingly loss is sometimes a compromise between two needs: our goal with the model and the ability of the function to be optimized using its gradient.\nWhat is the function to calculate new weights using a learning rate?\n\nnew_weight = old_weight - gradient*learning_rate\n\nWhat does the DataLoader class do? A DataLoader allows any python collection to be treated as an iterator. It has built in functionality to create batches and allows for shuffling, which improves the performance of training, as it gives our model variety. Notice below that listing the dl multiple times will return different batches.\n\nfrom torch.utils.data import DataLoader\n&gt;&gt;&gt; coll = range(2,26)\n&gt;&gt;&gt; dl = DataLoader(coll, batch_size=4, shuffle=True)\n&gt;&gt;&gt; list(dl)\n[tensor([13, 17, 23, 16]), tensor([ 4,  2, 12, 15]), tensor([ 5, 22, 10, 19]), tensor([ 7, 25, 21, 24]), tensor([ 6, 20,  8,  9]), tensor([ 3, 11, 18, 14])]\n&gt;&gt;&gt; list(dl)\n[tensor([17, 16, 22, 24]), tensor([21,  6, 12,  5]), tensor([14,  8,  4, 20]), tensor([15, 13, 18,  9]), tensor([11,  7, 23, 25]), tensor([19,  2,  3, 10])]\n&gt;&gt;&gt; list(dl)\n[tensor([12, 25, 24,  4]), tensor([ 5, 22,  2,  3]), tensor([13, 21, 23, 14]), tensor([10, 17,  6, 18]), tensor([15,  8, 19, 20]), tensor([11, 16,  7,  9])]\n\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n\nfor each opoch:\n  - for each batch\n    - make a prediction\n    - calculate the loss\n    - calculate the gradients\n    - updates the parameters based on the gradients * lr\n\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\ndef special(a, b):\n  return list(zip(a,b))\nThis structure is how pythorch expects to receive datasets. The first item in the tuple is the independant variable. The second item is the dependent variable. In other words these are the inputs and the targets of the model.\n\nWhat does view do in PyTorch? View allows pytorch to change the shape of a tensor without changing its contents. The new shape has to be compatible with the existing contents. For example a tensor([1,2,3,4]) can be reshaped into a tensor([[1,2],[3,4]]) because the number of elements is the same.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10))\n&gt;&gt;&gt; a\ntensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; a.view(3,3)\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\nWhat are the ‚Äúbias‚Äù parameters in a neural network? Why do we need them? The formula for a line is \\(y=w*x+b\\). Bias is the \\(b\\) in this formula. This allows our function to be more flexible, as in input of zero won‚Äôt always have to result in an output of zero.\nWhat does the @ operator do in Python? The @ operator performs matrix multiplication.\nWhat does the backward method do? The backward method calculates the gradients input parameters WRT the output of loss function. It does this by following the chain rule. It calculates the gradient of the loss function with respect to the output of the model, then the gradient of the output of the model with respect to the input parameters. It does this by following the computation graph that was created when the model was run. In order to be able to do this, the input params tensor need to have requires_grad set to True.\nWhy do we have to zero the gradients? We do this after we update the input parameters. This is because the gradients are accumulated. In other words, if we don‚Äôt zero them, they will continue to increase. We want to start fresh with each batch.\nWhat information do we have to pass to Learner? We have to pass the data, the model, the loss function, and the optimizer function (ie SGD), and optionally a metric function.\nShow Python or pseudocode for the basic steps of a training loop.\n\n- unpack our dl\n- calculate the loss, which implies making a prediciton with the model and comparing it to the actual label\n- calculate the gradients\n- update the parameters by param -= gradients * lr\n- zero the gradients\n\nWhat is ‚ÄúReLU‚Äù? Draw a plot of it for values from -2 to +2. A nonlinear function. For negative values its output is zero. For positive values its output is the input value.\nWhat is an ‚Äúactivation function‚Äù? These are the outputs of a layer in a nueral network.\nWhat‚Äôs the difference between F.relu and nn.ReLU? F.relu is a function. nn.ReLU is a class. They both do the same thing, but nn.ReLU is an object that can be used in a model.\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? We use more because it allows us to have fewer input parameters, which makese the model faster to train.\n\nextra credit üòè:\n\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter. kaggle notebook"
  },
  {
    "objectID": "posts/lesson 3/post.html#quiz",
    "href": "posts/lesson 3/post.html#quiz",
    "title": "FastAI Lesson 3",
    "section": "",
    "text": "How is a grayscale image represented on a computer? How about a color image? Images are represented as matrixes in a computer, with every cell in the matrix representing a pixel. In a grayscale image, the value of the cell is the intensity of the pixel. In a color image, the value of the cell is a vector of three values, representing the intensity of the red, green, and blue channels of the pixel.\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why? This dataset has a csv of labels and Train and ‚Äòvalid‚Äô folder. Within each folder, there is a subfolder for each label, eg /train/3. The data is organized this way because this is a common structure and allows for easy loading (dataLoader can read the parent folder and infer the labels from the subfolders).\nExplain how the ‚Äúpixel similarity‚Äù approach to classifying digits works. This approach approach takes the matrix of threes and calculates a mean matrix. We then compare a given image to this mean image. The comparison is performed using something a function that handles negative numbers but taking the absolute values or squaring the squaring the values. This is necessary because simple summing of the matrix resulting from taking the difference will cause some numbers to cancel each other out. The operation to deal with negative numbers (squaring or absolute value) is performed before calculating the mean. The result of this operation is a single number, which is the ‚Äúdistance‚Äù between the image and the mean image. The image is then classified as the label of the mean image with the smallest distance.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them. A list comprehension is a handy way of dealing with lists in python. It allows you to perform an operation on each element of a list and return a new list. For example, [x^2 for x in range(10) if x%2==1]will return a list of the odd numbers from 0 to 9, doubled.\nWhat is a ‚Äúrank-3 tensor‚Äù? A rank-3 tensor is a tensor with 3 dimensions. A rank-0 tensor is a single number, also called a scalar. A rank-1 tensor is a vector. A rank-2 tensor is a matrix. A rank-3 tensor is a 3-dimensional matrix, meaning a matrix of matrixes.\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape? Rank is the number of dimensions in a tensor. Shape is the size of each dimension. You can get the rank from the shape by counting the number of elements in the shape. For example Tensor([1,2,3,4]).shape will return torch.Size([4]) which has a rank 1.\n\n&gt;&gt;&gt; tensor([1],[1],[1]).shape\ntorch.Size([3, 1]) # rank 2, three rows, one column\n# stacked_threes is a a rank 3 tensor\n# it has shape torch.Size([6131, 28, 28]) \nstacked_threes[0][0][0].ndim # rank 0, ie scalar, ie raw value\nstacked_threes[0][0].ndim # rank 1, ie vector\nstacked_threes[0].ndim # rank 2, ie matrix\nstacked_threes.ndim # rank 3, ie 3-dimensional matrix\n\nWhat are RMSE and L1 norm? These are loss functions. RMSE is the root mean squared error. It is the square root of the mean of the squared differences between the predictions and the actual values. The L1 norm is the mean of the absolute value of the differences between the predictions and the actual values.\n\n# RMSE\ndef rmse(preds, targets):\n    return ((preds-targets)**2).mean().sqrt()\n# L1 norm\ndef l1(preds, targets):\n    return (preds-targets).abs().mean()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Using matrix multiplication. Matrix multiplication is a way of performing a calculation on a matrix of numbers. It is much faster than a python loop because it is implemented in C, which is much faster than python. The operation happens on the GPU, which is designed for this kind of operation.\nCreate a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10)).reshape(3,3)*2\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n&gt;&gt;&gt; a[1:,1:]\ntensor([[10, 12],\n        [16, 18]])\n\nWhat is broadcasting? Broadcasting is the ability of pytorch to perform operations on tensors of different shapes. For example, if you have a rank-1 tensor and a rank-2 tensor, pytorch will automatically expand the rank-1 tensor to match the shape of the rank-2 tensor. This is useful because it allows you to perform operations on tensors of different shapes without having to manually reshape them.\n\n&gt;&gt;&gt; a = torch.tensor([1,2,3])\n&gt;&gt;&gt; b = torch.tensor([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; a+b\ntensor([[2, 4, 6],\n        [5, 7, 9]])\n&gt;&gt;&gt; a.broadcast_to((2,3))\ntensor([[1, 2, 3],\n        [1, 2, 3]])\n&gt;&gt;&gt; b.broadcast_to((1,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]]])\n&gt;&gt;&gt; b.broadcast_to((2,2,3))\ntensor([[[1, 2, 3],\n         [4, 5, 6]],\n        [[1, 2, 3],\n         [4, 5, 6]]])\n\nAre metrics generally calculated using the training set, or the validation set? Why? Metrics are calculated using the validation set. We do this because we want to make sure the model is training to become generalized. If we used the training set, we would be optimizing the model to perform well on the training set, but we wouldn‚Äôt know if it was generalizing well.\nWhat is SGD? SGD is a way of optimizing a function, say for examples \\(y=x^2\\). SGD is a way of finding the minimum of this function. It does this by using the derivative to calculate the gradient, ie the slope at a particular point. The gradient tells us how changing the input parameters will change the output. If the gradient is positive, then increasing the input will increase the output. If the gradient is negative, then increasing the input will decrease the output. The gradient is calculated at a particular point, and then the input is adjusted by a small amount in the opposite direction of the gradient. This is repeated until the gradient is zero, which means that the input is at the minimum of the function. We are trying to minimize the function in our case because the function represents the loss of our model ie the gap between our model‚Äôs prediction and the actual value of the input.\nWhy does SGD use mini-batches? We use mini-batches in order to speed up the calculation in SGD. When we are optimizing, we are usually doing so over a large number of parameters. If we were to calculate the gradient for each parameter, it would take a long time. Instead, we can calculate the gradient for a small batch of parameters (as an average of the items in the batch), and then use that to adjust the parameters in the batch. This is much faster than calculating the gradient for each parameter individually.\nWhat are the seven steps in SGD for machine learning?\n\nInitialize the parameters with random values\nCalculate the predictions\nCalculate the loss\nCalculate the gradients, which approximates how the paramters need to change to reduce the loss\nAdjust the parameters by a small amount in the opposite direction of the gradient. ie If the gradient is positive, then decrease the parameter. If the gradient is negative, then increase the parameter.\nRepeat steps 2-5 until the loss is small enough for our purposes\n\nHow do we initialize the weights in a model? Randomly. That‚Äôs the ‚Äústochastic‚Äù part of SGD. We initialize the weights randomly because we don‚Äôt know what the optimal weights are. We are trying to find them. So we start with random weights and then use SGD to adjust them.\nWhat is ‚Äúloss‚Äù? Loss is a calculation of the difference between the model‚Äôs prediction and the actual value. It is a way of measuring how well the model is performing.\nWhy can‚Äôt we always use a high learning rate? If the learning rate is too high, we will bounce our parameters around, possible never arriving at the minimum. If the learning rate is too low, it will take a long time to arrive at the minimum.\nWhat is a ‚Äúgradient‚Äù? A gradient is a calculation of the slope of a function at a particular point. It tells us how changing the input parameters will change the output. Its the actual specific value of the derivative at a particular point in the loss function.\nDo you need to know how to calculate gradients yourself? Nope, pytorch does it for us.\nWhy can‚Äôt we use accuracy as a loss function? Accuracy is generally steppy. Meaning that an improvement of 0.1% of our model a big deal, but might not immediately result in flipping one incorrect prediction to correct. But we want to be able to make small adjustments to our parameters. So we need a loss function that is smooth, meaning that small changes in the parameters will result in small changes in the loss. Smoothness also allows us to easily find gradients and derivatives.\nDraw the sigmoid function. What is special about its shape? Sigmoid takes all inputs and normalizes them into a value between 0 and 1. This allows easy optimization using SGD.\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nWhat is the difference between a loss function and a metric? A loss function has to be smooth, meaning that small changes in the parameters cause a response in the loss. Smoothness also allows us to easily find gradients and derivatives. A metric doesn‚Äôt have to be smooth. It just has to be a way of measuring how well the model is performing. A metric, on the other hand, is really what we care about. We care about accuracy, but this might not be amendable to optimization using SGD. As a human, we want to focus on this, rather than loss, in judging our model‚Äôs performance. The computer doing the optimizing, on the other hand, will use the loss to do its work. Interestingly loss is sometimes a compromise between two needs: our goal with the model and the ability of the function to be optimized using its gradient.\nWhat is the function to calculate new weights using a learning rate?\n\nnew_weight = old_weight - gradient*learning_rate\n\nWhat does the DataLoader class do? A DataLoader allows any python collection to be treated as an iterator. It has built in functionality to create batches and allows for shuffling, which improves the performance of training, as it gives our model variety. Notice below that listing the dl multiple times will return different batches.\n\nfrom torch.utils.data import DataLoader\n&gt;&gt;&gt; coll = range(2,26)\n&gt;&gt;&gt; dl = DataLoader(coll, batch_size=4, shuffle=True)\n&gt;&gt;&gt; list(dl)\n[tensor([13, 17, 23, 16]), tensor([ 4,  2, 12, 15]), tensor([ 5, 22, 10, 19]), tensor([ 7, 25, 21, 24]), tensor([ 6, 20,  8,  9]), tensor([ 3, 11, 18, 14])]\n&gt;&gt;&gt; list(dl)\n[tensor([17, 16, 22, 24]), tensor([21,  6, 12,  5]), tensor([14,  8,  4, 20]), tensor([15, 13, 18,  9]), tensor([11,  7, 23, 25]), tensor([19,  2,  3, 10])]\n&gt;&gt;&gt; list(dl)\n[tensor([12, 25, 24,  4]), tensor([ 5, 22,  2,  3]), tensor([13, 21, 23, 14]), tensor([10, 17,  6, 18]), tensor([15,  8, 19, 20]), tensor([11, 16,  7,  9])]\n\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n\nfor each opoch:\n  - for each batch\n    - make a prediction\n    - calculate the loss\n    - calculate the gradients\n    - updates the parameters based on the gradients * lr\n\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\ndef special(a, b):\n  return list(zip(a,b))\nThis structure is how pythorch expects to receive datasets. The first item in the tuple is the independant variable. The second item is the dependent variable. In other words these are the inputs and the targets of the model.\n\nWhat does view do in PyTorch? View allows pytorch to change the shape of a tensor without changing its contents. The new shape has to be compatible with the existing contents. For example a tensor([1,2,3,4]) can be reshaped into a tensor([[1,2],[3,4]]) because the number of elements is the same.\n\n&gt;&gt;&gt; a = torch.tensor(range(1,10))\n&gt;&gt;&gt; a\ntensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; a.view(3,3)\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\nWhat are the ‚Äúbias‚Äù parameters in a neural network? Why do we need them? The formula for a line is \\(y=w*x+b\\). Bias is the \\(b\\) in this formula. This allows our function to be more flexible, as in input of zero won‚Äôt always have to result in an output of zero.\nWhat does the @ operator do in Python? The @ operator performs matrix multiplication.\nWhat does the backward method do? The backward method calculates the gradients input parameters WRT the output of loss function. It does this by following the chain rule. It calculates the gradient of the loss function with respect to the output of the model, then the gradient of the output of the model with respect to the input parameters. It does this by following the computation graph that was created when the model was run. In order to be able to do this, the input params tensor need to have requires_grad set to True.\nWhy do we have to zero the gradients? We do this after we update the input parameters. This is because the gradients are accumulated. In other words, if we don‚Äôt zero them, they will continue to increase. We want to start fresh with each batch.\nWhat information do we have to pass to Learner? We have to pass the data, the model, the loss function, and the optimizer function (ie SGD), and optionally a metric function.\nShow Python or pseudocode for the basic steps of a training loop.\n\n- unpack our dl\n- calculate the loss, which implies making a prediciton with the model and comparing it to the actual label\n- calculate the gradients\n- update the parameters by param -= gradients * lr\n- zero the gradients\n\nWhat is ‚ÄúReLU‚Äù? Draw a plot of it for values from -2 to +2. A nonlinear function. For negative values its output is zero. For positive values its output is the input value.\nWhat is an ‚Äúactivation function‚Äù? These are the outputs of a layer in a nueral network.\nWhat‚Äôs the difference between F.relu and nn.ReLU? F.relu is a function. nn.ReLU is a class. They both do the same thing, but nn.ReLU is an object that can be used in a model.\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? We use more because it allows us to have fewer input parameters, which makese the model faster to train.\n\nextra credit üòè:\n\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter. kaggle notebook"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Giant Morons",
    "section": "",
    "text": "FastAI Lesson 4\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 4\n\n\n\n\n\n\nNov 20, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nA Checkin on what‚Äôs working\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nSome observations and thoughts about my learning process so far\n\n\n\n\n\n\nNov 17, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 3\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 3\n\n\n\n\n\n\nNov 15, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 2\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for lesson 2\n\n\n\n\n\n\nNov 13, 2023\n\n\nZander Mackie\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Week 1\n\n\n\n\n\n\n\nfastai\n\n\nPart 1\n\n\n\n\nRecap, quiz, and sharing post for week 1\n\n\n\n\n\n\nNov 7, 2023\n\n\nZander Mackie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Giant Morons: a blog about deep learning üî¨",
    "section": "",
    "text": "Computers, as any programmer will tell you, are giant morons, not giant brains - Arthur Samuel\n\nThoughts, notes, experiments, and useful tidbits I find in my deep learning journey.\nI‚Äôm Zander, a father, a learner, and ever-curious software engineer. I‚Äôm captivated by recent AI developments. This blog aims to document my journey from zero to ML/deep learning practioner.\nAlong the way I‚Äôll be sharing useful things I learn and publicly documenting of my work. I‚Äôll also be making bad jokes. Hope you‚Äôre okay with that.\nThe banner image for this blog was created with dall-e"
  },
  {
    "objectID": "posts/week1/post.html",
    "href": "posts/week1/post.html",
    "title": "FastAI Week 1",
    "section": "",
    "text": "This week was basically an intro and some quick examples of using the fastai library in a variety of contexts. Most of the initial demonstration comes in the form of computer vision.\nI ran through the clean notebook on Kaggle. I also made my own image classifier that evaluates pictures of bean leaves and labels them with various disease type (or if they‚Äôre healthy). \n\n\nI thought it would be valuable to go through the quiz and document my answers and reasoning (if that applies).\n\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\n\nThese are all false, although common, misconceptions. I‚Äôd say I had these too for a long time. This course convinced me otherwise.\n\nName five areas where deep learning is now the best in the world.\n\nradiology\ngo playing\n\nWhat was the name of the first device that was based on the principle of the artificial neuron? perception\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\n\na set of processing units (what is this?)\na state of activation\nan output function for each unit\na pattern of connectvity (I‚Äôm taking all the above to basically be analagous to a perceptron)\npropogation (how do we ‚Äúpush‚Äù activities into the model to train it)\nactivation rule (how multiple produce an output)\nlearning rule (how do we modify a network of perceptrons by ‚Äúexperience‚Äù)\nenvironment\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\n\npeople missed minsky‚Äôs second insight that although a single layer could not approximate functions, multiple layers could\nhowever multilayered networks were often too slow to be useful (until GPUs)\n\n\nWhat is a GPU?\n\n\nA matrix multiplying chip (originballyt designed for graphics)\n\n\nOpen a notebook and execute a cell containing: 1+1. What happens? 2!\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. https://www.kaggle.com/code/zanadar/fastai-ch1-clean\nComplete the Jupyter Notebook online appendix. Don‚Äôt know what this is‚Ä¶.\nWhy is it hard to use a traditional computer program to recognize images in a photo? How would you describe the steps of this process? ‚Äúcomputers are giant morons, not giant brains‚Äù\nWhat did Samuel mean by ‚Äúweight assignment‚Äù? weight assignments are the value of variables (weights) that are part of the input to our ML program. they affect its operation.\nWhat term do we normally use in deep learning for what Samuel called ‚Äúweights‚Äù? parameters\nDraw a picture that summarizes Samuel‚Äôs view of a machine learning model. \nWhy is it hard to understand why a deep learning model makes a particular prediction? Sheer volume of model interconnections, parameters. This is called interprability\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? The universal approximation theorem\nWhat do you need in order to train a model? Labeled data\nHow could a feedback loop impact the rollout of a predictive policing model? Arrests/area -&gt; activity is concentrated in that area -&gt; more arrests there. This is the problem of ‚Äúoverfitting‚Äù and happens when a model doesn‚Äôt generalize/\nDo we always have to use 224√ó224-pixel images with the cat recognition model? No but thats a convention\nWhat is the difference between classification and regression? classification: categories. regression: numerical value\nWhat is a validation set? What is a test set? Why do we need them? validation is used to check the model after training and as part of the process of creating the model to make sure our model generalized. Test set is a further ‚Äòsecret‚Äô set of data that is used to check the model at the end.\nWhat will fastai do if you don‚Äôt provide a validation set? Create one on its own\nCan we always use a random sample for a validation set? Why or why not? No.¬†For example in time series we can‚Äôt just randomly select points as they won‚Äôt make sense for the input. we would set aside a validation set thats a portion of the timeline.\nWhat is overfitting? Provide an example. When a model is trained for a long time on data, it can learn to recognize the exact data in the training set and not generalize. For example it would recognize all the individual cats, but not generalze to a new cat.\nWhat is a metric? How does it differ from ‚Äúloss‚Äù? an indication of performance of the model against the validation data. loss can be the same, but it might differ in order to be useful as a function to update the parameters/weights\nHow can pretrained models help? Taking a model where there are existing layers that can do something and specializign the last layer(s) to our specific task. For example earlier layers may be able to recognize shapes, edges, textures, etc. and then we use transfer learning to tune that model to recognize cats and dogs.\nWhat is the ‚Äúhead‚Äù of a model? the top layer\nWhat kinds of features do the early layers of a CNN find? How about the later layers? In a vision model, more basic shapes like edges and gradients. Later is more specific elaborations of those earlier layers, like ‚Äúmultiple layers‚Äù or patterns\nAre image models only useful for photos? No.¬†Different kinds of data can be turned into images and then used to train a model.\nWhat is an ‚Äúarchitecture‚Äù? An organziation of layers within a model, albeit as a template for a function. A model is an architecture + a set of parameters,\nWhat is segmentation? Labelling pixels in an imge.\nWhat is y_range used for? When do we need it? in a collabarative model, with a regression prediction, y_range tells us the range of possible predictions\nWhat are ‚Äúhyperparameters‚Äù? Choices about parameters themselves (such as learning rates and data augmentation strategies), that lend meaning to the weights\nWhat‚Äôs the best way to avoid failures when using AI in an organization? Have a good test dataset to check a model against."
  },
  {
    "objectID": "posts/week1/post.html#quiz",
    "href": "posts/week1/post.html#quiz",
    "title": "FastAI Week 1",
    "section": "",
    "text": "I thought it would be valuable to go through the quiz and document my answers and reasoning (if that applies).\n\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\n\nThese are all false, although common, misconceptions. I‚Äôd say I had these too for a long time. This course convinced me otherwise.\n\nName five areas where deep learning is now the best in the world.\n\nradiology\ngo playing\n\nWhat was the name of the first device that was based on the principle of the artificial neuron? perception\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\n\na set of processing units (what is this?)\na state of activation\nan output function for each unit\na pattern of connectvity (I‚Äôm taking all the above to basically be analagous to a perceptron)\npropogation (how do we ‚Äúpush‚Äù activities into the model to train it)\nactivation rule (how multiple produce an output)\nlearning rule (how do we modify a network of perceptrons by ‚Äúexperience‚Äù)\nenvironment\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\n\npeople missed minsky‚Äôs second insight that although a single layer could not approximate functions, multiple layers could\nhowever multilayered networks were often too slow to be useful (until GPUs)\n\n\nWhat is a GPU?\n\n\nA matrix multiplying chip (originballyt designed for graphics)\n\n\nOpen a notebook and execute a cell containing: 1+1. What happens? 2!\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. https://www.kaggle.com/code/zanadar/fastai-ch1-clean\nComplete the Jupyter Notebook online appendix. Don‚Äôt know what this is‚Ä¶.\nWhy is it hard to use a traditional computer program to recognize images in a photo? How would you describe the steps of this process? ‚Äúcomputers are giant morons, not giant brains‚Äù\nWhat did Samuel mean by ‚Äúweight assignment‚Äù? weight assignments are the value of variables (weights) that are part of the input to our ML program. they affect its operation.\nWhat term do we normally use in deep learning for what Samuel called ‚Äúweights‚Äù? parameters\nDraw a picture that summarizes Samuel‚Äôs view of a machine learning model. \nWhy is it hard to understand why a deep learning model makes a particular prediction? Sheer volume of model interconnections, parameters. This is called interprability\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? The universal approximation theorem\nWhat do you need in order to train a model? Labeled data\nHow could a feedback loop impact the rollout of a predictive policing model? Arrests/area -&gt; activity is concentrated in that area -&gt; more arrests there. This is the problem of ‚Äúoverfitting‚Äù and happens when a model doesn‚Äôt generalize/\nDo we always have to use 224√ó224-pixel images with the cat recognition model? No but thats a convention\nWhat is the difference between classification and regression? classification: categories. regression: numerical value\nWhat is a validation set? What is a test set? Why do we need them? validation is used to check the model after training and as part of the process of creating the model to make sure our model generalized. Test set is a further ‚Äòsecret‚Äô set of data that is used to check the model at the end.\nWhat will fastai do if you don‚Äôt provide a validation set? Create one on its own\nCan we always use a random sample for a validation set? Why or why not? No.¬†For example in time series we can‚Äôt just randomly select points as they won‚Äôt make sense for the input. we would set aside a validation set thats a portion of the timeline.\nWhat is overfitting? Provide an example. When a model is trained for a long time on data, it can learn to recognize the exact data in the training set and not generalize. For example it would recognize all the individual cats, but not generalze to a new cat.\nWhat is a metric? How does it differ from ‚Äúloss‚Äù? an indication of performance of the model against the validation data. loss can be the same, but it might differ in order to be useful as a function to update the parameters/weights\nHow can pretrained models help? Taking a model where there are existing layers that can do something and specializign the last layer(s) to our specific task. For example earlier layers may be able to recognize shapes, edges, textures, etc. and then we use transfer learning to tune that model to recognize cats and dogs.\nWhat is the ‚Äúhead‚Äù of a model? the top layer\nWhat kinds of features do the early layers of a CNN find? How about the later layers? In a vision model, more basic shapes like edges and gradients. Later is more specific elaborations of those earlier layers, like ‚Äúmultiple layers‚Äù or patterns\nAre image models only useful for photos? No.¬†Different kinds of data can be turned into images and then used to train a model.\nWhat is an ‚Äúarchitecture‚Äù? An organziation of layers within a model, albeit as a template for a function. A model is an architecture + a set of parameters,\nWhat is segmentation? Labelling pixels in an imge.\nWhat is y_range used for? When do we need it? in a collabarative model, with a regression prediction, y_range tells us the range of possible predictions\nWhat are ‚Äúhyperparameters‚Äù? Choices about parameters themselves (such as learning rates and data augmentation strategies), that lend meaning to the weights\nWhat‚Äôs the best way to avoid failures when using AI in an organization? Have a good test dataset to check a model against."
  },
  {
    "objectID": "posts/learning observations/post.html",
    "href": "posts/learning observations/post.html",
    "title": "A Checkin on what‚Äôs working",
    "section": "",
    "text": "Observations on my learning process\n\nTenacious animal\n\n\n\nDall-e can‚Äôt spell\n\n\n\n\nThe learning process\nI thought it might be useful a this point to note a few things about my process of going through the course, so far. In addtiton to the content of the course, fastai is just brilliant in encorporating practices of learning that are miles more effective than the typical bottom up approach. There are also small ways the course does this, which are fascinating to try and pick out.\nAt the outset, knowing that my time was limited, it was important to me to optimize my learning. There are a number of resource that I drew on for this:\n\nBarbara Oakley‚Äôs course on learning how to learn\nMarcin, a fastai alums book on learning\nScott Young‚Äôs book on Ultralearning\nThe book ‚ÄúMake it Stick‚Äù\nThe book ‚ÄúThe First 20 Hours‚Äù\nThe paper ‚ÄúTeaching the Science of Learning‚Äù\n\nThat latter paper talks about 6 techniques that are effective for learning, which I‚Äôll elaborate on below:\n\nSpaced practice\nRetrieval practice\nElaboration\nInterleaving\nConcrete examples\nDual coding\n\nSpaced practice is relatively known (anki).\nRetrieval practice is suprising, in some ways. We learn by trying to retrieve information. And in fact actually quizzing yourself on information that you know nothing about somehow primes your brain to learn better. Jeremy has a number of structural elements in the course that are designed to do this. The quizzes at the end of each lesson are one example. The other is the ‚Äúfastai‚Äù library itself. It‚Äôs a library that is designed to be used by people who don‚Äôt know anything about deep learning.\nI‚Äôve been trying to use chatgpt to quiz me about the chapters‚Ä¶and its not great at specifics. In general (higher level) it gives good questions. Quizzing is a big part of it. I know try and go through the quizzes before, during, and after lessons. I also try and do the clean notebooks, quzzing myeslef on the outputs. Another technique I haven‚Äôt tried, but would like to, is doing a sort of ‚Äúflipped‚Äù approach to the clean notebooks, where I try and create the code based on the prose descriptions.\nInterleaving is a technique I haven‚Äôt tried. Basically you weave subjects together. Like I could weave vision and tabular DL learning practice.\nConcrete examples are obvious, and are how the course is rooted.\nDual coding is about presenting the information in multiple forms, like visual and written. It isn‚Äôt about learning styles, but about complementary information.\nI‚Äôve also been experimenting with using various LLM tools to both enhance my learning and my coding productivity. For example the learner I developed for the last lesson used copilot and chatgpt to help create the classes, do some refactoring, and some debugging. I‚Äôve also been trying to use a remote jupyter kernel with a local notebook instance so I can utilize copilot as part of my authoring process. I think I‚Äôll be experimenting with nbdev next.\nAnyway this is a quick dump of some techniques and experiments I‚Äôve been trying. I‚Äôll write later to see how they are working."
  }
]